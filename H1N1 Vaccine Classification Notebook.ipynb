{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import (LabelEncoder, FunctionTransformer,\n",
    "                                   OneHotEncoder)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import (BaseEstimator, TransformerMixin)\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             classification_report, precision_score,\n",
    "                             make_scorer)\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Pandemics have occured throughout history, each time affecting a large portion of the population. The most recent case being the Covid-19 2020 outbreak which resulted in a prolonged change in peoples day to day life, a lack or resources, and ultimately many deaths all around the world. If possible governments will do their best to try and prevent any future outbreaks by observing and learning from past information.     \n",
    "\n",
    "Therefore our stakeholder, a government agency, is trying to plan for future pandemic prevention and awareness by using the 2009 H1N1 pandemic as an example. They would like us to observe which features of a survey completed at the time appear to hold the highest importance in people who did not recieve the H1N1 vaccine. With this information they are hoping to be able to concentrate their efforts to provide vaccination information and flu prevention methods to a group that was otherwise more succeptible to contracting the virus. These efforts will be performed in hopes of increasing the vaccination rates and to help limit the spread of future viruses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "1. Determine business objectives: You should first “thoroughly understand, from a business perspective, what the customer really wants to accomplish.” (CRISP-DM Guide) and then define business success criteria.\n",
    "2. Assess situation: Determine resources availability, project requirements, assess risks and contingencies, and conduct a cost-benefit analysis.\n",
    "3. Determine data mining goals: In addition to defining the business objectives, you should also define what success looks like from a technical data mining perspective.\n",
    "4. Produce project plan: Select technologies and tools and define detailed plans for each project phase.\n",
    "\n",
    "Our stakeholders goal is to help in the prevention of future pandemics by spreading vaccination and prevention information. To increase the value of their efforts they would like to target the population that are less inclined to receive the vaccination. The most successful outcome would be the prevention of a widespread outbreak resulting in a pandemic, but more realisticly an increase in vaccination awareness that results in a higher rate of people receiving vaccinations in case another outbreak occurs.\n",
    "\n",
    "Therefore this projects requirements are to define the target audience who are less inclined to receive a vaccination. In order to do so a data on people who have both received and refrained from receiving vaccinations in the past is required. Available to us is a survey conducted for the 2009 H1N1 pandemic. The H1N1 outbreak was first detected in the United States and quickly spread across the rest of the world resuliting in between 151,000 and 575,000 deaths worldwide. Unlike prior strains of the H1N1 virus, people under 65 were more affected than the older population. Around 80 percent of the deaths assumed caused by this strain of H1N1 were people under the age of 65. Since this strain differend from previous strains the seasonal flu vaccinations didn't offer protection from the virus causing a late production of a vaccine that would be affective. An affective vaccination didn't get mass produced until after a second wave of the virus had come and gone in the United States <a href=\"#h1n1_cdc_article\">[1]</a>.\n",
    "\n",
    "Due to these factors this dataset may stray from a typical case, reason being: \n",
    "\n",
    "1. <strong>The late emergence of the mass produced vaccine.</strong> It wasn't until after the second outbreak had passed that it was available, possibly causing people to assume the worst was over and a vaccination wasn't required and lowering the number of vaccinations.\n",
    "\n",
    "2. <strong>The age group most affected were people under 65.</strong> This isn't typical for outbreaks and may have caused the number of vaccinations in this age group to be inflated.\n",
    "\n",
    "Though our dataset may not reflect these two points, they should be kept in mind while analyzing the results. As far as metrics to qualify our models performance, I believe accuracy and precision should be used. A result of having high precision will ensure that our feature importance will strongly relate to true positive entries. Though there should be a fine balance and recall shouldn't be completely forgone. While we don't want our predicted positives to contain many false positives, we would like to predict a good portion of out true positive entries. By approaching our problem by utilizing our metrics in this manner we can assure that our stakeholder is targeting the correct audience.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "As mentioned earlier in the business understanding section, the data being used for this analysis will be a 2009 survey conducted for the H1N1 outbreak. This survey was performed by the CDC in order to monitor and evaluate the flu vaccination efforts amount adults and children. The participants were parts of randomly called US households. The questions asked the participants dealt with their H1N1 vaccination status, flu-related behaviors, opinions about flue vaccine safety and effectivenss, recent respiratory illness, and pneumococcal vaccination status <a href=\"#About the National Immunization Survery\">[2]</a>.\n",
    "\n",
    "A detailed explanation of the features included in this survey can be found <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1%20and%20Seasonal%20Flu%20Vaccines%20Feature%20Information.txt\">here</a> or <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/\">here</a>.\n",
    "<!--  link need to fitted to github repository when pushed -->\n",
    "\n",
    "The following data from the survey can be found <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/data/\">here</a> <a href=\"#Source Data Download\">[3]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import survey data into dataframes\n",
    "# The source dataset already had this split feature and target files\n",
    "X = pd.read_csv('data/source_data/training_set_features.csv')\n",
    "y = pd.read_csv('data/source_data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training_set_labels.csv` file read to `y` contains two target variables, `h1n1_vaccine` and `seasonal_vaccine1`. For this project only the `h1n1_vaccine` target variable will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets target variable\n",
    "y = y.h1n1_vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21033\n",
      "1     5674\n",
      "Name: h1n1_vaccine, dtype: int64\n",
      "0    0.787546\n",
      "1    0.212454\n",
      "Name: h1n1_vaccine, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checks distribution of target variable\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 79% of those who were surveyed did not receive the vaccination. \n",
    "\n",
    "Let't take a look at the featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>income_poverty</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...             income_poverty  marital_status  \\\n",
       "0                    1.0  ...              Below Poverty     Not Married   \n",
       "1                    1.0  ...              Below Poverty     Not Married   \n",
       "2                    0.0  ...  <= $75,000, Above Poverty     Not Married   \n",
       "3                    0.0  ...              Below Poverty     Not Married   \n",
       "4                    1.0  ...  <= $75,000, Above Poverty         Married   \n",
       "\n",
       "   rent_or_own   employment_status  hhs_geo_region                census_msa  \\\n",
       "0          Own  Not in Labor Force        oxchjgsf                   Non-MSA   \n",
       "1         Rent            Employed        bhuqouqj  MSA, Not Principle  City   \n",
       "2          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "3         Rent  Not in Labor Force        lrircsnp       MSA, Principle City   \n",
       "4          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "\n",
       "   household_adults  household_children  employment_industry  \\\n",
       "0               0.0                 0.0                  NaN   \n",
       "1               0.0                 0.0             pxcmvdjn   \n",
       "2               2.0                 0.0             rucpziij   \n",
       "3               0.0                 0.0                  NaN   \n",
       "4               1.0                 0.0             wxleyezf   \n",
       "\n",
       "   employment_occupation  \n",
       "0                    NaN  \n",
       "1               xgwztkwe  \n",
       "2               xtkaffoo  \n",
       "3                    NaN  \n",
       "4               emcorrxb  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview of feature dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the `respondent_id` column, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26707 entries, 0 to 26706\n",
      "Data columns (total 35 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   h1n1_concern                 26615 non-null  float64\n",
      " 1   h1n1_knowledge               26591 non-null  float64\n",
      " 2   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 3   behavioral_avoidance         26499 non-null  float64\n",
      " 4   behavioral_face_mask         26688 non-null  float64\n",
      " 5   behavioral_wash_hands        26665 non-null  float64\n",
      " 6   behavioral_large_gatherings  26620 non-null  float64\n",
      " 7   behavioral_outside_home      26625 non-null  float64\n",
      " 8   behavioral_touch_face        26579 non-null  float64\n",
      " 9   doctor_recc_h1n1             24547 non-null  float64\n",
      " 10  doctor_recc_seasonal         24547 non-null  float64\n",
      " 11  chronic_med_condition        25736 non-null  float64\n",
      " 12  child_under_6_months         25887 non-null  float64\n",
      " 13  health_worker                25903 non-null  float64\n",
      " 14  health_insurance             14433 non-null  float64\n",
      " 15  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 16  opinion_h1n1_risk            26319 non-null  float64\n",
      " 17  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 18  opinion_seas_vacc_effective  26245 non-null  float64\n",
      " 19  opinion_seas_risk            26193 non-null  float64\n",
      " 20  opinion_seas_sick_from_vacc  26170 non-null  float64\n",
      " 21  age_group                    26707 non-null  object \n",
      " 22  education                    25300 non-null  object \n",
      " 23  race                         26707 non-null  object \n",
      " 24  sex                          26707 non-null  object \n",
      " 25  income_poverty               22284 non-null  object \n",
      " 26  marital_status               25299 non-null  object \n",
      " 27  rent_or_own                  24665 non-null  object \n",
      " 28  employment_status            25244 non-null  object \n",
      " 29  hhs_geo_region               26707 non-null  object \n",
      " 30  census_msa                   26707 non-null  object \n",
      " 31  household_adults             26458 non-null  float64\n",
      " 32  household_children           26458 non-null  float64\n",
      " 33  employment_industry          13377 non-null  object \n",
      " 34  employment_occupation        13237 non-null  object \n",
      "dtypes: float64(23), object(12)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# drops respondent_id column\n",
    "X.drop('respondent_id', axis=1, inplace=True)\n",
    "\n",
    "# preview at feature column information\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datasets contains 26,707 entries and 35 features. The features have been inputted as either float64 or object types, 23 and 12 columns of those types respectively. As mentioned in the <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1%20and%20Seasonal%20Flu%20Vaccines%20Feature%20Information.txt\">feature description</a> file, the float64 column types are either encoded or binary where Yes=1 and No=0. It can also be observed from the above information that there are many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h1n1_concern                      92\n",
       "h1n1_knowledge                   116\n",
       "behavioral_antiviral_meds         71\n",
       "behavioral_avoidance             208\n",
       "behavioral_face_mask              19\n",
       "behavioral_wash_hands             42\n",
       "behavioral_large_gatherings       87\n",
       "behavioral_outside_home           82\n",
       "behavioral_touch_face            128\n",
       "doctor_recc_h1n1                2160\n",
       "doctor_recc_seasonal            2160\n",
       "chronic_med_condition            971\n",
       "child_under_6_months             820\n",
       "health_worker                    804\n",
       "health_insurance               12274\n",
       "opinion_h1n1_vacc_effective      391\n",
       "opinion_h1n1_risk                388\n",
       "opinion_h1n1_sick_from_vacc      395\n",
       "opinion_seas_vacc_effective      462\n",
       "opinion_seas_risk                514\n",
       "opinion_seas_sick_from_vacc      537\n",
       "age_group                          0\n",
       "education                       1407\n",
       "race                               0\n",
       "sex                                0\n",
       "income_poverty                  4423\n",
       "marital_status                  1408\n",
       "rent_or_own                     2042\n",
       "employment_status               1463\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "household_adults                 249\n",
       "household_children               249\n",
       "employment_industry            13330\n",
       "employment_occupation          13470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks amount of Nan values in feature dataframe\n",
    "missing_values = X.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all columns are missing values, and some are missing nearly half of their values missing. These columns may be dropped later while the others are filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving columns with many missing entries for later\n",
    "many_missing = missing_values[missing_values > 12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check correlation between our target variable and features. We will be using a Chi2 test, with alpha=.05 and our null hypothesis being that there is no relationship between our feature and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the feature and target variable\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the feature and target variable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_chi2(features, target='features', ascending=True, alpha=None):\n",
    "    '''\n",
    "    Returnes the ordered chi2 P-values between each feature and the target variable or variables\n",
    "    \n",
    "    Inputs: features = Dataframe of features\n",
    "            target = pd.Series as target or default = 'features' which results in \n",
    "            finding relationships inside features DataFrame\n",
    "            ascending = Determines order, default value = True\n",
    "            alpha = Chi2 Pvalue threshold for which features get returned.\n",
    "                        Returns all features with a Pvalue<=alpha,\n",
    "                        default = None so all features are returned\n",
    "            \n",
    "    Output: Dataframe containing ordered P-values\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(columns=['var1', 'var2', 'Pvalue'])\n",
    "                 \n",
    "    if isinstance(target, pd.Series):\n",
    "        for col in features.columns:\n",
    "            temp_dict={}\n",
    "            temp_dict['var1'] = target.name\n",
    "            temp_dict['var2'] = col\n",
    "            temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(target, features[col]))[1]\n",
    "            df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "    elif target == 'features':\n",
    "        combs = combinations(features.columns, 2)\n",
    "        for comb in combs:\n",
    "            temp_dict={}\n",
    "            temp_dict['var1'] = comb[0]\n",
    "            temp_dict['var2'] = comb[1]\n",
    "            temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(features[comb[0]], features[comb[1]]))[1]\n",
    "            df = df.append(temp_dict, ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        sys.exit('''Incorrect input for parameter target.\n",
    "        Parameter only accepts types pd.DataFrame, pd.Series, or left to default value.''')        \n",
    "    \n",
    "    if alpha == None:\n",
    "        return df.sort_values(by='Pvalue', ascending=ascending)\n",
    "    else:\n",
    "        return df[df.Pvalue <= alpha].sort_values(by='Pvalue', ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>Pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_seas_risk</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_h1n1_risk</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_h1n1_vacc_effective</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>doctor_recc_h1n1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>doctor_recc_seasonal</td>\n",
       "      <td>7.176678e-237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_seas_vacc_effective</td>\n",
       "      <td>1.719836e-218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>employment_industry</td>\n",
       "      <td>5.786809e-175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>employment_occupation</td>\n",
       "      <td>2.021881e-172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>health_worker</td>\n",
       "      <td>4.345311e-164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>h1n1_knowledge</td>\n",
       "      <td>8.631658e-87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>h1n1_concern</td>\n",
       "      <td>1.091885e-86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_h1n1_sick_from_vacc</td>\n",
       "      <td>2.389411e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>chronic_med_condition</td>\n",
       "      <td>1.486270e-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>health_insurance</td>\n",
       "      <td>7.934475e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_wash_hands</td>\n",
       "      <td>3.954447e-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_touch_face</td>\n",
       "      <td>1.929485e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_face_mask</td>\n",
       "      <td>1.528632e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>child_under_6_months</td>\n",
       "      <td>6.172942e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>education</td>\n",
       "      <td>1.706407e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>income_poverty</td>\n",
       "      <td>2.711955e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>marital_status</td>\n",
       "      <td>4.277018e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>hhs_geo_region</td>\n",
       "      <td>5.188916e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_avoidance</td>\n",
       "      <td>9.454102e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>age_group</td>\n",
       "      <td>2.971603e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>race</td>\n",
       "      <td>2.555777e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_antiviral_meds</td>\n",
       "      <td>4.319995e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>opinion_seas_sick_from_vacc</td>\n",
       "      <td>1.903307e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>rent_or_own</td>\n",
       "      <td>5.892166e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>household_adults</td>\n",
       "      <td>1.100713e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>employment_status</td>\n",
       "      <td>5.472123e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_outside_home</td>\n",
       "      <td>4.060957e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>sex</td>\n",
       "      <td>7.709155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>behavioral_large_gatherings</td>\n",
       "      <td>3.826512e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>household_children</td>\n",
       "      <td>4.585274e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>h1n1_vaccine</td>\n",
       "      <td>census_msa</td>\n",
       "      <td>9.445420e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            var1                         var2         Pvalue\n",
       "19  h1n1_vaccine            opinion_seas_risk   0.000000e+00\n",
       "16  h1n1_vaccine            opinion_h1n1_risk   0.000000e+00\n",
       "15  h1n1_vaccine  opinion_h1n1_vacc_effective   0.000000e+00\n",
       "9   h1n1_vaccine             doctor_recc_h1n1   0.000000e+00\n",
       "10  h1n1_vaccine         doctor_recc_seasonal  7.176678e-237\n",
       "18  h1n1_vaccine  opinion_seas_vacc_effective  1.719836e-218\n",
       "33  h1n1_vaccine          employment_industry  5.786809e-175\n",
       "34  h1n1_vaccine        employment_occupation  2.021881e-172\n",
       "13  h1n1_vaccine                health_worker  4.345311e-164\n",
       "1   h1n1_vaccine               h1n1_knowledge   8.631658e-87\n",
       "0   h1n1_vaccine                 h1n1_concern   1.091885e-86\n",
       "17  h1n1_vaccine  opinion_h1n1_sick_from_vacc   2.389411e-54\n",
       "11  h1n1_vaccine        chronic_med_condition   1.486270e-52\n",
       "14  h1n1_vaccine             health_insurance   7.934475e-48\n",
       "5   h1n1_vaccine        behavioral_wash_hands   3.954447e-34\n",
       "8   h1n1_vaccine        behavioral_touch_face   1.929485e-31\n",
       "4   h1n1_vaccine         behavioral_face_mask   1.528632e-30\n",
       "12  h1n1_vaccine         child_under_6_months   6.172942e-27\n",
       "22  h1n1_vaccine                    education   1.706407e-26\n",
       "25  h1n1_vaccine               income_poverty   2.711955e-17\n",
       "26  h1n1_vaccine               marital_status   4.277018e-16\n",
       "29  h1n1_vaccine               hhs_geo_region   5.188916e-16\n",
       "3   h1n1_vaccine         behavioral_avoidance   9.454102e-15\n",
       "21  h1n1_vaccine                    age_group   2.971603e-14\n",
       "23  h1n1_vaccine                         race   2.555777e-12\n",
       "2   h1n1_vaccine    behavioral_antiviral_meds   4.319995e-11\n",
       "20  h1n1_vaccine  opinion_seas_sick_from_vacc   1.903307e-09\n",
       "27  h1n1_vaccine                  rent_or_own   5.892166e-08\n",
       "31  h1n1_vaccine             household_adults   1.100713e-06\n",
       "28  h1n1_vaccine            employment_status   5.472123e-06\n",
       "7   h1n1_vaccine      behavioral_outside_home   4.060957e-04\n",
       "24  h1n1_vaccine                          sex   7.709155e-04\n",
       "6   h1n1_vaccine  behavioral_large_gatherings   3.826512e-03\n",
       "32  h1n1_vaccine           household_children   4.585274e-01\n",
       "30  h1n1_vaccine                   census_msa   9.445420e-01"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_chi2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe tells us that all but two features have failed the null hypothesis (H<sub>0</sub>) and are significant to our target variable. The two features that ended up being insignificant are `household_children` and `census_msa`.\n",
    "\n",
    "The top six features in terms of significance were:\n",
    "\n",
    "- `opinion_seas_risk`\n",
    "- `opinion_h1n1_risk`\n",
    "- `opinion_h1n1_vacc_effective`\n",
    "- `doctor_recc_h1n1`\n",
    "- `doctor_recc_seasonal`\n",
    "- `opinion_seas_vacc_effective`\n",
    "\n",
    "A theme of the top six is that there are basically three topics, asked each about the H1N1 flu and Seasonal flu. The initial assumption is that these pairs may be highly correlated. Let's confirm this by checking the correlation of all features using the Chi2 test again with an alpha of .05. The hypothesis are similar to the previous test between features and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the two features\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the two features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>Pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>employment_industry</td>\n",
       "      <td>employment_occupation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>race</td>\n",
       "      <td>hhs_geo_region</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>health_worker</td>\n",
       "      <td>employment_industry</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>education</td>\n",
       "      <td>employment_occupation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>education</td>\n",
       "      <td>employment_industry</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>child_under_6_months</td>\n",
       "      <td>opinion_seas_vacc_effective</td>\n",
       "      <td>0.030892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>behavioral_avoidance</td>\n",
       "      <td>employment_status</td>\n",
       "      <td>0.035881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>behavioral_antiviral_meds</td>\n",
       "      <td>marital_status</td>\n",
       "      <td>0.042447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>health_worker</td>\n",
       "      <td>marital_status</td>\n",
       "      <td>0.044544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>doctor_recc_h1n1</td>\n",
       "      <td>race</td>\n",
       "      <td>0.047821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          var1                         var2    Pvalue\n",
       "594        employment_industry        employment_occupation  0.000000\n",
       "534                       race               hhs_geo_region  0.000000\n",
       "383              health_worker          employment_industry  0.000000\n",
       "528                  education        employment_occupation  0.000000\n",
       "527                  education          employment_industry  0.000000\n",
       "..                         ...                          ...       ...\n",
       "347       child_under_6_months  opinion_seas_vacc_effective  0.030892\n",
       "123       behavioral_avoidance            employment_status  0.035881\n",
       "90   behavioral_antiviral_meds               marital_status  0.042447\n",
       "376              health_worker               marital_status  0.044544\n",
       "283           doctor_recc_h1n1                         race  0.047821\n",
       "\n",
       "[573 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=.05\n",
    "chi2_df = ordered_chi2(X, alpha=alpha)\n",
    "chi2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the DataFrame above, many features are correlated. There are a total of 573 instances of high correlation between features. Considering we have many missing values to deal with, the high correlation between certain features may help us fill in some missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Now that we have a better understanding of our data, let's prepare it for our models. \n",
    "\n",
    "We'll start with dropping columns and row containing many missing entries. We already found the columns that fit this criteria earlier. While we are performing this all duplicate entries will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of entries: 26707\n",
      "Number of entries after dropped Duplicates: 26707\n"
     ]
    }
   ],
   "source": [
    "X_prep = X.copy()\n",
    "\n",
    "X_prep.drop(many_missing.index, axis=1, inplace=True)\n",
    "\n",
    "# Drops duplicate entries if any exist\n",
    "df_all = pd.concat([X_prep, y], axis=1).drop_duplicates()\n",
    "X_prep = df_all.drop(y.name, axis=1)\n",
    "y_prep = df_all[y.name]\n",
    "\n",
    "print(f'Original number of entries: {X.shape[0]}')\n",
    "print(f'Number of entries after dropped Duplicates: {X_prep.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There weren't any duplicate entries.\n",
    "\n",
    "Before we start editing more values and rows lets create a train-test split to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training and test splits to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y_prep, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how many rows are missing many values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_row_entries(df):\n",
    "    '''\n",
    "    Prints a DataFrame where:\n",
    "        index = num of nan entries in a row\n",
    "        frequency = number of rows with amount of nans defined by index\n",
    "        cum_sum = cumulative sum starting with rows with most missing entries\n",
    "        \n",
    "    inputs: df = DataFrame with missing entries\n",
    "    output: DataFrame, with nan per row information\n",
    "    '''\n",
    "    \n",
    "    # Creates a list with number of missing values in each row\n",
    "    nan_per_row = []\n",
    "    for i in range(df.shape[0]):\n",
    "        nan_per_row.append(df.iloc[i,:].isna().sum())\n",
    "       \n",
    "    # creates dataseries of missing values\n",
    "    nan_per_row_ds = pd.Series(nan_per_row)\n",
    "    nan_per_row_ds.rename('frequency', inplace=True)\n",
    "    \n",
    "    # gets frequeny of nan amounts in rows\n",
    "    nan_row_counts = nan_per_row_ds.value_counts()\n",
    "    \n",
    "#     Orders nan_row_counts descending by most nans down\n",
    "    ordered_missing_nan = nan_row_counts[nan_row_counts.keys().sort_values(ascending=False)]\n",
    "    \n",
    "#     Creates dataseries with cumulative sum\n",
    "    nan_cum_sum = np.cumsum(ordered_missing_nan)   \n",
    "    nan_cum_sum.rename('cum_sum', inplace=True)\n",
    "    \n",
    "#     Creates DataFrame with rows per missing values amount, and cum sum\n",
    "    nan_df = pd.concat([ordered_missing_nan, nan_cum_sum], axis=1)\n",
    "    nan_df.rename_axis('num_of_nans_in_row', inplace=True)\n",
    "    \n",
    "    return nan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>cum_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_of_nans_in_row</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>103</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>90</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>39</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>106</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>288</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1694</td>\n",
       "      <td>3110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2176</td>\n",
       "      <td>5286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14744</td>\n",
       "      <td>20030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    frequency  cum_sum\n",
       "num_of_nans_in_row                    \n",
       "19                          2        2\n",
       "18                         32       34\n",
       "17                         11       45\n",
       "16                        103      148\n",
       "15                         13      161\n",
       "14                         90      251\n",
       "13                         28      279\n",
       "12                         35      314\n",
       "11                         39      353\n",
       "10                         73      426\n",
       "9                          39      465\n",
       "8                         106      571\n",
       "7                          67      638\n",
       "6                          31      669\n",
       "5                         288      957\n",
       "4                         101     1058\n",
       "3                         358     1416\n",
       "2                        1694     3110\n",
       "1                        2176     5286\n",
       "0                       14744    20030"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find row missing nan information\n",
    "missing_row_entries(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current features dataframe has 32 features after dropping those four columns. That means around 353 entries are missing about a third (11 values) of their features information. Since these entries are missing most of their information lets drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_by_nans(df, y, nan_threshold=None):\n",
    "    '''\n",
    "    Drop rows by quantity of nan values with nan_threshold as cut-off point\n",
    "    \n",
    "    Inputs: \n",
    "            df = DataFrame to be altered, should contain features and target concatenated together\n",
    "            y = target variable column name\n",
    "            nan_threshold = cut-off point to drop rows, default = None which results in a value\n",
    "                            of half or a little larger than number of features\n",
    "    \n",
    "    Outputs: \n",
    "            feature_df = feature DataFrame\n",
    "            y = Target Variable\n",
    "    '''\n",
    "    \n",
    "#     Checks value of nan_threshold\n",
    "    if nan_threshold == None:\n",
    "#         Sets to half of or rounded up from half of feature columns\n",
    "        nan_threshold = math.ceil((len(df.columns)-1)/2)\n",
    "    \n",
    "#     Finds nans contained in each row \n",
    "    nan_per_row = []\n",
    "    for i in range(df.shape[0]):\n",
    "        nan_per_row.append(df.iloc[i,:].isna().sum())\n",
    "    \n",
    "#     Creates temporary column in df for number of nan values\n",
    "    df['nans'] = nan_per_row\n",
    "#     Creates dataframe of rows under nan threshold\n",
    "    df = df[df.nans < nan_threshold]\n",
    "#     Creates target variable\n",
    "    y = df.h1n1_vaccine\n",
    "#     returns tuple (feature_df, target)\n",
    "    return (df.drop(['nans', 'h1n1_vaccine'], axis=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates custom fuction transfromer\n",
    "Many_Nans_Row_Drop_FT = FunctionTransformer(drop_rows_by_nans, kw_args = {'y': 'h1n1_vaccine',\n",
    "                                                                          'nan_threshold': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates training set of both features and target to be transformed\n",
    "train_df = pd.concat([X_train, y_train], axis=1).copy()\n",
    "\n",
    "# Transforms training set\n",
    "X_train_mod, y_train_mod = Many_Nans_Row_Drop_FT.fit_transform(train_df)\n",
    "\n",
    "# Checks change in shape to ensure proper amount of entries were dropped\n",
    "X_train.shape[0] - X_train_mod.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try and fill in the rest of our missing values. Earlier we noticed that many variables had high colinearity with each other, we can use this to help us fill in for missing values. Our first attempt at filling the values will be through classification using the other features.\n",
    "\n",
    "Issue with this process is the existence of missing values, many models in scikit-learn libraries cannot handle Nan values. Therefore random imputation will be used to fill in the feature information. The values will be randomly selected using existing values and their rate of appearances in the features existing values. After that is completed an iterative Decision Tree Classifier will be used to fill the randomly imputed values with predicted values.\n",
    "\n",
    "This process was modified from Shashanka Subrahmanya's process for a regression model in his article <a href=\"https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression\">Missing Data Imputation using Regression</a> <a href='#Missing Data Imputation using Regression'>[4]</a>. His process utilized linear regression and didn't create custom transfomers. For the purpose of this project I have modified it to fit my needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomImputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Randomly imputes values for missing data in new columns.\n",
    "    Values are based off of existing values and rates of occurences.\n",
    "    \n",
    "    Initialized with optional parameter columns which mark which\n",
    "    columns will be transformed, if left to default value 'all', all\n",
    "    columns with missing values will be filled.\n",
    "    '''\n",
    "    \n",
    "#     Initializes class object\n",
    "    def __init__(self, missing_columns='all_missing_columns'):\n",
    "        self.missing_columns = missing_columns\n",
    "\n",
    "#     If columns equals all_missing_columns sets attribute to all missing columns \n",
    "    def fit(self, X, y=None):\n",
    "        if self.missing_columns == 'all_missing_columns':\n",
    "            nan_amount = X.isna().sum()\n",
    "            self.missing_columns = list(nan_amount[nan_amount>0].index)\n",
    "#     Handles if single column entered as string\n",
    "        elif type(self.missing_columns) == str:\n",
    "            self.missing_columns = [self.missing_columns]\n",
    "        \n",
    "        feature_value_info = {}\n",
    "        for col in self.missing_columns:\n",
    "            feature_value_info[col] = X.loc[X[col].notnull(), col].value_counts(normalize=True)\n",
    "            \n",
    "        self.feature_value_info = feature_value_info\n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        for col in self.missing_columns:\n",
    "#     Creates copy of column to have calues imputed into\n",
    "            df[col+'_imp'] = df[col]\n",
    "#     Finds number of missing values in column\n",
    "            number_missing = df[col].isnull().sum()\n",
    "#     Randomly Imputes observed values replacing all missing information\n",
    "            df.loc[df[col].isnull(), col+'_imp'] = np.random.choice(self.feature_value_info[col].index, \n",
    "                                                                    number_missing, \n",
    "                                                                    replace = True,\n",
    "                                                                    p = self.feature_value_info[col])\n",
    "\n",
    "#     Creates column index variable to be called to set DataFrame index\n",
    "        self.features_out = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "#     Returns final columns index\n",
    "    def get_features_out(self):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will create a RandomImputer object, fit and transform it to our training feature dataset, and then check if all the missing values have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group                          0\n",
       "race                               0\n",
       "sex                                0\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "h1n1_concern_imp                   0\n",
       "h1n1_knowledge_imp                 0\n",
       "behavioral_antiviral_meds_imp      0\n",
       "behavioral_avoidance_imp           0\n",
       "behavioral_face_mask_imp           0\n",
       "behavioral_wash_hands_imp          0\n",
       "behavioral_large_gatherings_imp    0\n",
       "behavioral_outside_home_imp        0\n",
       "behavioral_touch_face_imp          0\n",
       "doctor_recc_h1n1_imp               0\n",
       "doctor_recc_seasonal_imp           0\n",
       "chronic_med_condition_imp          0\n",
       "child_under_6_months_imp           0\n",
       "health_worker_imp                  0\n",
       "opinion_h1n1_vacc_effective_imp    0\n",
       "opinion_h1n1_risk_imp              0\n",
       "opinion_h1n1_sick_from_vacc_imp    0\n",
       "opinion_seas_vacc_effective_imp    0\n",
       "opinion_seas_risk_imp              0\n",
       "opinion_seas_sick_from_vacc_imp    0\n",
       "education_imp                      0\n",
       "income_poverty_imp                 0\n",
       "marital_status_imp                 0\n",
       "rent_or_own_imp                    0\n",
       "employment_status_imp              0\n",
       "household_adults_imp               0\n",
       "household_children_imp             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates class object\n",
    "rand_imp = RandomImputer()\n",
    "\n",
    "# Fits and transforms dataset\n",
    "X_train_imp = rand_imp.fit_transform(X_train_mod)\n",
    "\n",
    "# checks if all columns besides original columns with missing data are filled in\n",
    "X_train_imp.drop(rand_imp.missing_columns, axis=1).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each feature has been filled with randomly imputed data, it's time to apply a regression models to predict the values for missing data from the other features data instead of randomly imputing the data.\n",
    "\n",
    "This requires an iterative process for each feature that was originally missing data. Since each column is categorical they will need to be OHE for each iteration and the target variable (column missing data) will need to be treated as a multiclass classification problem. Also since the features contain various amount of information, I would like to iterate through the columns filling by number of missing values. I am not sure if filling in the columns with little data missing first, or a lot of missing data is better. By filling in the columns with less missing data first, it could help achieve more accurate information to fill the columns with many missing entries. On the other hand filling in the columns with many missing values, will take out more randomness for future iterations, by filling in more random values with predicted values from classification. To start I believe I will try the method of handling columns with many missing entries first. Also since many columns and iterations are needed Decision Trees will be used to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeClassification(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth=None, missing_columns='all_missing_columns', class_order='many_first'):\n",
    "        self.max_depth=max_depth\n",
    "        self.missing_columns=missing_columns\n",
    "        self.class_order=class_order\n",
    "        \n",
    "#     If columns equals all_missing_columns sets attribute to all missing columns\n",
    "#     missing_columns parameter will be in order of iteration\n",
    "    def fit(self, X, y=None):\n",
    "        if self.missing_columns == 'all_missing_columns':\n",
    "            nan_amount = X.isna().sum()\n",
    "            if self.class_order == 'many_first':\n",
    "                self.missing_columns = list(nan_amount[nan_amount>0].sort_values(ascending=False).index)\n",
    "            elif self.class_order == 'less_first':\n",
    "                self.missing_columns = list(nan_amount[nan_amount>0].sort_values().index)\n",
    "            else:\n",
    "                sys.exit('''Incorrect input for class_order parameter.\n",
    "                    Parameter only accepts values ('many_first', or 'less_first')''')\n",
    "#     Handles if single column entered as string\n",
    "        elif type(self.missing_columns) == str:\n",
    "            self.missing_columns = [self.missing_columns]\n",
    "\n",
    "        leftover_features = list(set(X.columns) - set(self.missing_columns) - \n",
    "                                {col+'_imp' for col in self.missing_columns})\n",
    "        \n",
    "        pred_features = [col+'_imp' for col in self.missing_columns] + leftover_features\n",
    "        pred_df = X[pred_features].copy()\n",
    "#         pred_df = pd.DataFrame(columns = ['Det_'+col for col in self.missing_columns])\n",
    "        models = {}\n",
    "        accuracy_scores = {}\n",
    "        \n",
    "        for col in self.missing_columns:\n",
    "#             pred_df['Det_'+col] = X[col+'_imp']\n",
    "            temp_features = list(set(pred_df.columns) - {col+'_imp'})\n",
    "            \n",
    "#             ohe = OneHotEncoder(sparse=False)\n",
    "            ohe = OneHotEncoder()\n",
    "            ohe_features = ohe.fit_transform(pred_df[temp_features])\n",
    "            \n",
    "            dt = DecisionTreeClassifier(max_depth=self.max_depth, random_state=42)\n",
    "            dt.fit(ohe_features, pred_df[col+'_imp'])\n",
    "            \n",
    "            pred_df.loc[X[col].isnull(), col+'_imp'] = dt.predict(ohe_features)[X[col].isnull()]\n",
    "            \n",
    "            models[col] = dt\n",
    "            accuracy_scores[col] = accuracy_score(X.loc[X[col].notnull(), col],\n",
    "                                                dt.predict(ohe_features)[X[col].notnull()])\n",
    "            \n",
    "        self.dt_models = models\n",
    "        self.fit_accuracy_scores = accuracy_scores\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        pred_df = pd.DataFrame(columns = ['Det_'+col for col in self.missing_columns])\n",
    "        accuracy_scores = {}\n",
    "        \n",
    "        for col in self.missing_columns:\n",
    "            pred_df['Det_'+col] = X[col+'_imp']\n",
    "            temp_features = list(set(X.columns) - set(self.missing_columns) - {col+'_imp'})\n",
    "            \n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe_features = ohe.fit_transform(X[temp_features])\n",
    "            \n",
    "            dt_model = self.dt_models[col]\n",
    "            \n",
    "            pred_df.loc[X[col].isnull(), 'Det_'+col] = dt_model.predict(ohe_features)[X[col].isnull()]\n",
    "            \n",
    "            accuracy_scores[col] = accuracy_score(X.loc[X[col].notnull(), col],\n",
    "                                                  dt_model.predict(ohe_features)[X[col].notnull()])\n",
    "        \n",
    "        self.features_predicted = pred_df.columns\n",
    "        self.transformed_accuracy_scores = accuracy_scores\n",
    "        \n",
    "        leftover_features = list(set(X.columns) - set(self.missing_columns) - \n",
    "                                {col+'_imp' for col in self.missing_columns})\n",
    "        \n",
    "        self.features_out = pred_df.columns\n",
    "        \n",
    "        return pd.concat([pred_df, X[leftover_features]], axis=1)\n",
    "    \n",
    "    def get_features_out(self):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the IterativeClassificatin class object is creatred, fit and the accuracy scores are checked. The accuracy score was checked with each iteration with the first entry being the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'income_poverty': 0.6585525923274268,\n",
       " 'doctor_recc_h1n1': 0.8502338376891334,\n",
       " 'doctor_recc_seasonal': 0.8260247592847317,\n",
       " 'rent_or_own': 0.8030564855815963,\n",
       " 'employment_status': 0.731437015145918,\n",
       " 'education': 0.4693522906793049,\n",
       " 'marital_status': 0.851832350463353,\n",
       " 'chronic_med_condition': 0.7292357083678542,\n",
       " 'child_under_6_months': 0.9183526383526384,\n",
       " 'health_worker': 0.8889003241908094,\n",
       " 'behavioral_avoidance': 0.7867970910580764,\n",
       " 'behavioral_touch_face': 0.7594464869281046,\n",
       " 'h1n1_knowledge': 0.60464404184741,\n",
       " 'household_children': 0.7322485207100592,\n",
       " 'household_adults': 0.7368394205264232,\n",
       " 'opinion_h1n1_vacc_effective': 0.5750573833205815,\n",
       " 'behavioral_large_gatherings': 0.813166080261078,\n",
       " 'opinion_seas_sick_from_vacc': 0.5764543924947739,\n",
       " 'h1n1_concern': 0.5068562981087832,\n",
       " 'opinion_seas_risk': 0.5741156081149964,\n",
       " 'behavioral_outside_home': 0.8154434250764526,\n",
       " 'behavioral_antiviral_meds': 0.9531162411455945,\n",
       " 'opinion_seas_vacc_effective': 0.601639260805376,\n",
       " 'opinion_h1n1_risk': 0.5684108724422274,\n",
       " 'behavioral_wash_hands': 0.8554413112083885,\n",
       " 'opinion_h1n1_sick_from_vacc': 0.5741456468673718,\n",
       " 'behavioral_face_mask': 0.9323637103336045}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates IterativeClassification class object with max_depth = 5\n",
    "iter_class = IterativeClassification(max_depth=5)\n",
    "\n",
    "# Fits object to training data\n",
    "iter_class.fit(X_train_imp)\n",
    "\n",
    "# Checks accuracy model by checking predictions against existing values\n",
    "iter_class.fit_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the decision tree classifier didn't work great for all columns, it can definently be said that it performed better than random guessing. Let's transform our training set to fill in the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Det_income_poverty</th>\n",
       "      <th>Det_doctor_recc_h1n1</th>\n",
       "      <th>Det_doctor_recc_seasonal</th>\n",
       "      <th>Det_rent_or_own</th>\n",
       "      <th>Det_employment_status</th>\n",
       "      <th>Det_education</th>\n",
       "      <th>Det_marital_status</th>\n",
       "      <th>Det_chronic_med_condition</th>\n",
       "      <th>Det_child_under_6_months</th>\n",
       "      <th>Det_health_worker</th>\n",
       "      <th>...</th>\n",
       "      <th>Det_opinion_seas_vacc_effective</th>\n",
       "      <th>Det_opinion_h1n1_risk</th>\n",
       "      <th>Det_behavioral_wash_hands</th>\n",
       "      <th>Det_opinion_h1n1_sick_from_vacc</th>\n",
       "      <th>Det_behavioral_face_mask</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>12 Years</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>lzgpxyit</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>kbazzjca</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Male</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Det_income_poverty  Det_doctor_recc_h1n1  \\\n",
       "25194  <= $75,000, Above Poverty                   0.0   \n",
       "14006  <= $75,000, Above Poverty                   0.0   \n",
       "11285  <= $75,000, Above Poverty                   0.0   \n",
       "2900               Below Poverty                   0.0   \n",
       "19083  <= $75,000, Above Poverty                   1.0   \n",
       "\n",
       "       Det_doctor_recc_seasonal Det_rent_or_own Det_employment_status  \\\n",
       "25194                       0.0             Own    Not in Labor Force   \n",
       "14006                       1.0             Own              Employed   \n",
       "11285                       0.0             Own              Employed   \n",
       "2900                        0.0             Own              Employed   \n",
       "19083                       1.0             Own              Employed   \n",
       "\n",
       "          Det_education Det_marital_status  Det_chronic_med_condition  \\\n",
       "25194          12 Years        Not Married                        0.0   \n",
       "14006      Some College            Married                        1.0   \n",
       "11285  College Graduate        Not Married                        0.0   \n",
       "2900   College Graduate        Not Married                        1.0   \n",
       "19083  College Graduate        Not Married                        0.0   \n",
       "\n",
       "       Det_child_under_6_months  Det_health_worker  ...  \\\n",
       "25194                       0.0                0.0  ...   \n",
       "14006                       1.0                0.0  ...   \n",
       "11285                       0.0                0.0  ...   \n",
       "2900                        0.0                0.0  ...   \n",
       "19083                       0.0                0.0  ...   \n",
       "\n",
       "       Det_opinion_seas_vacc_effective  Det_opinion_h1n1_risk  \\\n",
       "25194                              4.0                    2.0   \n",
       "14006                              4.0                    2.0   \n",
       "11285                              4.0                    1.0   \n",
       "2900                               4.0                    1.0   \n",
       "19083                              1.0                    1.0   \n",
       "\n",
       "       Det_behavioral_wash_hands  Det_opinion_h1n1_sick_from_vacc  \\\n",
       "25194                        0.0                              2.0   \n",
       "14006                        1.0                              1.0   \n",
       "11285                        0.0                              1.0   \n",
       "2900                         0.0                              1.0   \n",
       "19083                        1.0                              2.0   \n",
       "\n",
       "       Det_behavioral_face_mask                census_msa     sex  \\\n",
       "25194                       0.0                   Non-MSA  Female   \n",
       "14006                       0.0  MSA, Not Principle  City  Female   \n",
       "11285                       0.0       MSA, Principle City  Female   \n",
       "2900                        0.0  MSA, Not Principle  City    Male   \n",
       "19083                       0.0  MSA, Not Principle  City  Female   \n",
       "\n",
       "           age_group  hhs_geo_region   race  \n",
       "25194  18 - 34 Years        oxchjgsf  White  \n",
       "14006  45 - 54 Years        lzgpxyit  White  \n",
       "11285  45 - 54 Years        kbazzjca  White  \n",
       "2900   55 - 64 Years        mlyzmhmf  White  \n",
       "19083  18 - 34 Years        bhuqouqj  White  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforms dataset to fill in missing values\n",
    "X_train_trans = iter_class.transform(X_train_imp)\n",
    "\n",
    "# print first 5 entries of transformed dataset\n",
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Det_income_poverty                 0\n",
       "Det_doctor_recc_h1n1               0\n",
       "Det_doctor_recc_seasonal           0\n",
       "Det_rent_or_own                    0\n",
       "Det_employment_status              0\n",
       "Det_education                      0\n",
       "Det_marital_status                 0\n",
       "Det_chronic_med_condition          0\n",
       "Det_child_under_6_months           0\n",
       "Det_health_worker                  0\n",
       "Det_behavioral_avoidance           0\n",
       "Det_behavioral_touch_face          0\n",
       "Det_h1n1_knowledge                 0\n",
       "Det_household_children             0\n",
       "Det_household_adults               0\n",
       "Det_opinion_h1n1_vacc_effective    0\n",
       "Det_behavioral_large_gatherings    0\n",
       "Det_opinion_seas_sick_from_vacc    0\n",
       "Det_h1n1_concern                   0\n",
       "Det_opinion_seas_risk              0\n",
       "Det_behavioral_outside_home        0\n",
       "Det_behavioral_antiviral_meds      0\n",
       "Det_opinion_seas_vacc_effective    0\n",
       "Det_opinion_h1n1_risk              0\n",
       "Det_behavioral_wash_hands          0\n",
       "Det_opinion_h1n1_sick_from_vacc    0\n",
       "Det_behavioral_face_mask           0\n",
       "census_msa                         0\n",
       "sex                                0\n",
       "age_group                          0\n",
       "hhs_geo_region                     0\n",
       "race                               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks dataframe for missing values\n",
    "X_train_trans.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now has no missing values. The features with `Det_` starting their names, are those that have been transformed to have their missing values filled in.\n",
    "\n",
    "Now that we have no missing values, we need to One Hot Encode all our columns. Since we are looking for feature importance no columns will be dropped during the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19677, 107)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates OneHotEncoder class object\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# fits and transforms ohe object to X_train_trans\n",
    "X_train_ohe = ohe.fit_transform(X_train_trans)\n",
    "\n",
    "# Creates DataFrame from nparray\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe,\n",
    "                           columns=ohe.get_feature_names_out(X_train_trans.columns),\n",
    "                           index=X_train_trans.index)\n",
    "\n",
    "# Checks shape of our new training feature data set\n",
    "X_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Det_income_poverty_&lt;= $75,000, Above Poverty</th>\n",
       "      <th>Det_income_poverty_&gt; $75,000</th>\n",
       "      <th>Det_income_poverty_Below Poverty</th>\n",
       "      <th>Det_doctor_recc_h1n1_0.0</th>\n",
       "      <th>Det_doctor_recc_h1n1_1.0</th>\n",
       "      <th>Det_doctor_recc_seasonal_0.0</th>\n",
       "      <th>Det_doctor_recc_seasonal_1.0</th>\n",
       "      <th>Det_rent_or_own_Own</th>\n",
       "      <th>Det_rent_or_own_Rent</th>\n",
       "      <th>Det_employment_status_Employed</th>\n",
       "      <th>...</th>\n",
       "      <th>hhs_geo_region_kbazzjca</th>\n",
       "      <th>hhs_geo_region_lrircsnp</th>\n",
       "      <th>hhs_geo_region_lzgpxyit</th>\n",
       "      <th>hhs_geo_region_mlyzmhmf</th>\n",
       "      <th>hhs_geo_region_oxchjgsf</th>\n",
       "      <th>hhs_geo_region_qufhixun</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Hispanic</th>\n",
       "      <th>race_Other or Multiple</th>\n",
       "      <th>race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Det_income_poverty_<= $75,000, Above Poverty  \\\n",
       "25194                                           1.0   \n",
       "14006                                           1.0   \n",
       "11285                                           1.0   \n",
       "2900                                            0.0   \n",
       "19083                                           1.0   \n",
       "\n",
       "       Det_income_poverty_> $75,000  Det_income_poverty_Below Poverty  \\\n",
       "25194                           0.0                               0.0   \n",
       "14006                           0.0                               0.0   \n",
       "11285                           0.0                               0.0   \n",
       "2900                            0.0                               1.0   \n",
       "19083                           0.0                               0.0   \n",
       "\n",
       "       Det_doctor_recc_h1n1_0.0  Det_doctor_recc_h1n1_1.0  \\\n",
       "25194                       1.0                       0.0   \n",
       "14006                       1.0                       0.0   \n",
       "11285                       1.0                       0.0   \n",
       "2900                        1.0                       0.0   \n",
       "19083                       0.0                       1.0   \n",
       "\n",
       "       Det_doctor_recc_seasonal_0.0  Det_doctor_recc_seasonal_1.0  \\\n",
       "25194                           1.0                           0.0   \n",
       "14006                           0.0                           1.0   \n",
       "11285                           1.0                           0.0   \n",
       "2900                            1.0                           0.0   \n",
       "19083                           0.0                           1.0   \n",
       "\n",
       "       Det_rent_or_own_Own  Det_rent_or_own_Rent  \\\n",
       "25194                  1.0                   0.0   \n",
       "14006                  1.0                   0.0   \n",
       "11285                  1.0                   0.0   \n",
       "2900                   1.0                   0.0   \n",
       "19083                  1.0                   0.0   \n",
       "\n",
       "       Det_employment_status_Employed  ...  hhs_geo_region_kbazzjca  \\\n",
       "25194                             0.0  ...                      0.0   \n",
       "14006                             1.0  ...                      0.0   \n",
       "11285                             1.0  ...                      1.0   \n",
       "2900                              1.0  ...                      0.0   \n",
       "19083                             1.0  ...                      0.0   \n",
       "\n",
       "       hhs_geo_region_lrircsnp  hhs_geo_region_lzgpxyit  \\\n",
       "25194                      0.0                      0.0   \n",
       "14006                      0.0                      1.0   \n",
       "11285                      0.0                      0.0   \n",
       "2900                       0.0                      0.0   \n",
       "19083                      0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_mlyzmhmf  hhs_geo_region_oxchjgsf  \\\n",
       "25194                      0.0                      1.0   \n",
       "14006                      0.0                      0.0   \n",
       "11285                      0.0                      0.0   \n",
       "2900                       1.0                      0.0   \n",
       "19083                      0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_qufhixun  race_Black  race_Hispanic  \\\n",
       "25194                      0.0         0.0            0.0   \n",
       "14006                      0.0         0.0            0.0   \n",
       "11285                      0.0         0.0            0.0   \n",
       "2900                       0.0         0.0            0.0   \n",
       "19083                      0.0         0.0            0.0   \n",
       "\n",
       "       race_Other or Multiple  race_White  \n",
       "25194                     0.0         1.0  \n",
       "14006                     0.0         1.0  \n",
       "11285                     0.0         1.0  \n",
       "2900                      0.0         1.0  \n",
       "19083                     0.0         1.0  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one hot encoding our feature training dataset we have 107 columns.\n",
    "\n",
    "Let's now bring our test sets up to date with the transformations. There were three transformations performed on he model.\n",
    "\n",
    "1. Dropping rows with many values\n",
    "2. Random Imputation for missing values\n",
    "3. Using a Decision Tree Classifier to help better predict those missing values\n",
    "4. One Hot Encode all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Det_income_poverty_&lt;= $75,000, Above Poverty</th>\n",
       "      <th>Det_income_poverty_&gt; $75,000</th>\n",
       "      <th>Det_income_poverty_Below Poverty</th>\n",
       "      <th>Det_doctor_recc_h1n1_0.0</th>\n",
       "      <th>Det_doctor_recc_h1n1_1.0</th>\n",
       "      <th>Det_doctor_recc_seasonal_0.0</th>\n",
       "      <th>Det_doctor_recc_seasonal_1.0</th>\n",
       "      <th>Det_rent_or_own_Own</th>\n",
       "      <th>Det_rent_or_own_Rent</th>\n",
       "      <th>Det_employment_status_Employed</th>\n",
       "      <th>...</th>\n",
       "      <th>hhs_geo_region_kbazzjca</th>\n",
       "      <th>hhs_geo_region_lrircsnp</th>\n",
       "      <th>hhs_geo_region_lzgpxyit</th>\n",
       "      <th>hhs_geo_region_mlyzmhmf</th>\n",
       "      <th>hhs_geo_region_oxchjgsf</th>\n",
       "      <th>hhs_geo_region_qufhixun</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Hispanic</th>\n",
       "      <th>race_Other or Multiple</th>\n",
       "      <th>race_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Det_income_poverty_<= $75,000, Above Poverty  \\\n",
       "16515                                           1.0   \n",
       "23353                                           0.0   \n",
       "10008                                           0.0   \n",
       "11635                                           1.0   \n",
       "19331                                           0.0   \n",
       "\n",
       "       Det_income_poverty_> $75,000  Det_income_poverty_Below Poverty  \\\n",
       "16515                           0.0                               0.0   \n",
       "23353                           1.0                               0.0   \n",
       "10008                           1.0                               0.0   \n",
       "11635                           0.0                               0.0   \n",
       "19331                           1.0                               0.0   \n",
       "\n",
       "       Det_doctor_recc_h1n1_0.0  Det_doctor_recc_h1n1_1.0  \\\n",
       "16515                       1.0                       0.0   \n",
       "23353                       1.0                       0.0   \n",
       "10008                       1.0                       0.0   \n",
       "11635                       0.0                       1.0   \n",
       "19331                       1.0                       0.0   \n",
       "\n",
       "       Det_doctor_recc_seasonal_0.0  Det_doctor_recc_seasonal_1.0  \\\n",
       "16515                           0.0                           1.0   \n",
       "23353                           1.0                           0.0   \n",
       "10008                           1.0                           0.0   \n",
       "11635                           0.0                           1.0   \n",
       "19331                           1.0                           0.0   \n",
       "\n",
       "       Det_rent_or_own_Own  Det_rent_or_own_Rent  \\\n",
       "16515                  1.0                   0.0   \n",
       "23353                  1.0                   0.0   \n",
       "10008                  1.0                   0.0   \n",
       "11635                  1.0                   0.0   \n",
       "19331                  1.0                   0.0   \n",
       "\n",
       "       Det_employment_status_Employed  ...  hhs_geo_region_kbazzjca  \\\n",
       "16515                             1.0  ...                      0.0   \n",
       "23353                             1.0  ...                      0.0   \n",
       "10008                             1.0  ...                      0.0   \n",
       "11635                             0.0  ...                      0.0   \n",
       "19331                             1.0  ...                      0.0   \n",
       "\n",
       "       hhs_geo_region_lrircsnp  hhs_geo_region_lzgpxyit  \\\n",
       "16515                      0.0                      0.0   \n",
       "23353                      0.0                      0.0   \n",
       "10008                      1.0                      0.0   \n",
       "11635                      0.0                      0.0   \n",
       "19331                      0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_mlyzmhmf  hhs_geo_region_oxchjgsf  \\\n",
       "16515                      0.0                      0.0   \n",
       "23353                      0.0                      0.0   \n",
       "10008                      0.0                      0.0   \n",
       "11635                      1.0                      0.0   \n",
       "19331                      0.0                      0.0   \n",
       "\n",
       "       hhs_geo_region_qufhixun  race_Black  race_Hispanic  \\\n",
       "16515                      0.0         0.0            0.0   \n",
       "23353                      0.0         0.0            0.0   \n",
       "10008                      0.0         0.0            0.0   \n",
       "11635                      0.0         0.0            0.0   \n",
       "19331                      0.0         0.0            0.0   \n",
       "\n",
       "       race_Other or Multiple  race_White  \n",
       "16515                     0.0         1.0  \n",
       "23353                     0.0         1.0  \n",
       "10008                     0.0         1.0  \n",
       "11635                     0.0         1.0  \n",
       "19331                     0.0         1.0  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combines test features and target for first transformation\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Drops rows with many missing entries\n",
    "X_test_mod, y_test_mod = Many_Nans_Row_Drop_FT.transform(test_df)\n",
    "\n",
    "# Fits and transforms dataset\n",
    "X_test_imp = rand_imp.transform(X_test_mod)\n",
    "\n",
    "# transforms dataset to fill in missing values\n",
    "X_test_trans = iter_class.transform(X_test_imp)\n",
    "\n",
    "# One hot transforms test feature set\n",
    "X_test_ohe = ohe.transform(X_test_trans)\n",
    "\n",
    "# Creates dataframe from nparray\n",
    "X_test_ohe = pd.DataFrame(X_test_ohe,\n",
    "                           columns=ohe.get_feature_names_out(X_test_trans.columns),\n",
    "                           index=X_test_trans.index)\n",
    "\n",
    "# Checks preview of tranformed test feature dataframe\n",
    "X_test_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Base Model\n",
    "\n",
    "Now that we have working datasets, let's begin the modeling process. Let's first create a basic model using a Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score: 0.8286324134776643\n",
      "Test Accuracy Score: 0.8287368581441414\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "dt.fit(X_train_ohe, y_train_mod)\n",
    "\n",
    "train_pred_dt = dt.predict(X_train_ohe) \n",
    "test_pre_dt = dt.predict(X_test_ohe)\n",
    "\n",
    "dt_train_score = dt.score(X_train_ohe, y_train_mod)\n",
    "dt_test_score = dt.score(X_test_ohe, y_test_mod)\n",
    "\n",
    "print(f'Train Accuracy Score: {dt_train_score}')\n",
    "print(f'Test Accuracy Score: {dt_test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our base model appears to have performed pretty well and isn't overfit. It performed around 5% better than if we guessed with the 78.7% majority that didn't receive the vaccine.\n",
    "\n",
    "Let's take a look at the confusion matrix and well some other metrics. For the purpose of project, when calculating metrics like recall and precision, the target values will be flipped so an entry of not vaccinated is a positive result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Displays confusion matrix and classification report\n",
    "    \n",
    "    Inputs: y_true = real labels\n",
    "            y_pred = predicted labels\n",
    "    '''\n",
    "    \n",
    "#     Generates confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "#     Creates and displays dataframe for easier viewing of confusion matrix\n",
    "    display(pd.DataFrame(data=cm, columns=['pred_0', 'pred_1'], index=['true_0', 'true_1']))    \n",
    "    print()\n",
    "#     creates classification_report dictionary\n",
    "    cr_dict = classification_report(y_true, test_pre_dt, output_dict=True)\n",
    "#     displays metrics for our non-vaccinated predictions\n",
    "    display(cr_dict['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_0</th>\n",
       "      <td>4875</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_1</th>\n",
       "      <td>834</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred_0  pred_1\n",
       "true_0    4875     290\n",
       "true_1     834     564"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8539148712559117,\n",
       " 'recall': 0.9438528557599225,\n",
       " 'f1-score': 0.8966341732573111,\n",
       " 'support': 5165}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prints confusion matrix and qualification metrics\n",
    "quick_metrics(y_test_mod, test_pre_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently our base model has a precision of .85, meaning out of all our non-vaccinated predictions, 85% of them were correct. Our recall was .94 meaning we captured 94% of entries that weren't vaccinated. Though as mentioned before we want to concentrate our model qualification off of our precision, so let's try to improve this with future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DecisionTreeClassifier parameter grid\n",
    "par_grid = {'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [3, 4, 5, 6, 7],\n",
    "            'min_samples_split': [.01, .025, .05, .1]}\n",
    "\n",
    "# Sets scores to be calculated\n",
    "# refit paramater for GridSearchCV sets scoring for best features\n",
    "scores = {'prec': 'precision',\n",
    "          'acc': 'accuracy',\n",
    "          'recall': 'recall',\n",
    "          'ROC': 'roc_auc'}\n",
    "\n",
    "# Creates GridSearchCV object\n",
    "clf = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "                   param_grid=par_grid,\n",
    "                   scoring=scores,\n",
    "                   return_train_score = True,\n",
    "                   refit='prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scoring method to work properly, the target variables values have to be flipped since we're interested in predicting those who are not vaccinated.\n",
    "\n",
    "Therefore the values will be as followed, the old values are shown for reference:\n",
    "\n",
    "&emsp;Old: (Vaccinated = 1, Not-Vaccinated=0)<br>\n",
    "&emsp;New: (Vaccinated = 0, Not-Vaccinated=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_flip(ds):\n",
    "    '''\n",
    "    Takes in a pd.Series with [0,1] values and flips them.\n",
    "    ex. 0 to 1, 1 to 0\n",
    "    '''\n",
    "    return (ds+1).mod(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flips values of labels\n",
    "y_train_flipped = binary_flip(y_train_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits training values to grid search object\n",
    "clf.fit(X_train_ohe, y_train_flipped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates DataFrame of results\n",
    "clf_df = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankings(data):\n",
    "    '''\n",
    "    Takes in GridSearchCV df column and returns scores mean\n",
    "    and ranking\n",
    "    '''\n",
    "    \n",
    "    cols = [ind for ind in data.index if (('rank' in ind) | \n",
    "                                      ('mean' in ind) &\n",
    "                                      ('time' not in ind))]\n",
    "    return data.loc[cols, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_scores(data, score, data_split='test'):\n",
    "    '''\n",
    "    Returns fold scores, mean score, std and rank if test\n",
    "    set is specified for specific scoring metric\n",
    "    \n",
    "    Inputs: data = columns of interest from GridSearchCV of \n",
    "                   type DataFrame\n",
    "            score = str, name of score given to GridSearchCV object\n",
    "            data_split = str, ('test', 'train'), default='test',\n",
    "                         determines which set of scores is returned\n",
    "    Output: DataFrame containing mean score, fold scores, std\n",
    "            and rank if test set is specified\n",
    "    '''\n",
    "#     creates string to find index entries\n",
    "    search_str = data_split + '_' + score \n",
    "    \n",
    "#     creates list of index names\n",
    "    cols = [ind for ind in data.index if (search_str in ind)]\n",
    "    \n",
    "#     returnes DataFrame with found index names\n",
    "    return data.loc[cols, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe of best scorer row and trasposes it to a column\n",
    "best_scorer = clf_df.loc[clf_df.rank_test_prec==1, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_test_prec</th>\n",
       "      <td>0.856744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_prec</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_prec</th>\n",
       "      <td>0.859813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_acc</th>\n",
       "      <td>0.822534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_acc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_acc</th>\n",
       "      <td>0.827336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_recall</th>\n",
       "      <td>0.930205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_recall</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_recall</th>\n",
       "      <td>0.932867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_ROC</th>\n",
       "      <td>0.813313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_ROC</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_ROC</th>\n",
       "      <td>0.825184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         25\n",
       "mean_test_prec     0.856744\n",
       "rank_test_prec            1\n",
       "mean_train_prec    0.859813\n",
       "mean_test_acc      0.822534\n",
       "rank_test_acc             1\n",
       "mean_train_acc     0.827336\n",
       "mean_test_recall   0.930205\n",
       "rank_test_recall         49\n",
       "mean_train_recall  0.932867\n",
       "mean_test_ROC      0.813313\n",
       "rank_test_ROC             2\n",
       "mean_train_ROC     0.825184"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings(best_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>split0_train_prec</th>\n",
       "      <td>0.86234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_prec</th>\n",
       "      <td>0.862987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_prec</th>\n",
       "      <td>0.863189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_prec</th>\n",
       "      <td>0.847917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_prec</th>\n",
       "      <td>0.862633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_prec</th>\n",
       "      <td>0.859813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_prec</th>\n",
       "      <td>0.00595545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           25\n",
       "split0_train_prec     0.86234\n",
       "split1_train_prec    0.862987\n",
       "split2_train_prec    0.863189\n",
       "split3_train_prec    0.847917\n",
       "split4_train_prec    0.862633\n",
       "mean_train_prec      0.859813\n",
       "std_train_prec     0.00595545"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_scores(best_scorer, 'prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old work, working up to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves X_train_mod to pickle file for ease to reload\n",
    "with open('data/temp_pickle_files/X_train_mod.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train_mod, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens X_train_mod pickle file\n",
    "with open('data/temp_pickle_files/X_train_mod.pickle', 'rb') as f:\n",
    "    X_train_mod = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'income_poverty': 0.5769942127558022,\n",
       " 'doctor_recc_h1n1': 0.8248143053645117,\n",
       " 'doctor_recc_seasonal': 0.8248143053645117,\n",
       " 'rent_or_own': 0.7610973107247003,\n",
       " 'employment_status': 0.7124914243495699,\n",
       " 'education': 0.39831490258030544,\n",
       " 'marital_status': 0.8014953664700927,\n",
       " 'chronic_med_condition': 0.7154101077050539,\n",
       " 'child_under_6_months': 0.9180952380952381,\n",
       " 'health_worker': 0.8861730046827562,\n",
       " 'behavioral_avoidance': 0.7619584144217966,\n",
       " 'behavioral_touch_face': 0.7438214869281046,\n",
       " 'h1n1_knowledge': 0.549630007655014,\n",
       " 'household_children': 0.7050091817996327,\n",
       " 'household_adults': 0.7243929810242807,\n",
       " 'opinion_h1n1_vacc_effective': 0.5121142565672022,\n",
       " 'behavioral_large_gatherings': 0.8114323593901381,\n",
       " 'opinion_seas_sick_from_vacc': 0.5147096313669505,\n",
       " 'h1n1_concern': 0.442524341132691,\n",
       " 'opinion_seas_risk': 0.41905393006422675,\n",
       " 'behavioral_outside_home': 0.8127420998980632,\n",
       " 'behavioral_antiviral_meds': 0.9517403047444326,\n",
       " 'opinion_seas_vacc_effective': 0.5674286005192689,\n",
       " 'opinion_h1n1_risk': 0.49694594319454344,\n",
       " 'behavioral_wash_hands': 0.8235773185381248,\n",
       " 'opinion_h1n1_sick_from_vacc': 0.49832180634662326,\n",
       " 'behavioral_face_mask': 0.9311432058584215}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates IterativeClassification class object with max_depth = 5\n",
    "train_IterClass = IterativeClassification(max_depth=1)\n",
    "\n",
    "# Fits object to training data\n",
    "train_IterClass.fit(X_train_imp)\n",
    "\n",
    "# Checks accuracy model by checking predictions against existing values\n",
    "train_IterClass.fit_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the decision tree classifier didn't work great for all columns, it can definently be said that it performed better than random guessing. Let's proceed with these models to predict our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = train_IterClass.transform(X_train_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Det_income_poverty</th>\n",
       "      <th>Det_doctor_recc_h1n1</th>\n",
       "      <th>Det_doctor_recc_seasonal</th>\n",
       "      <th>Det_rent_or_own</th>\n",
       "      <th>Det_employment_status</th>\n",
       "      <th>Det_education</th>\n",
       "      <th>Det_marital_status</th>\n",
       "      <th>Det_chronic_med_condition</th>\n",
       "      <th>Det_child_under_6_months</th>\n",
       "      <th>Det_health_worker</th>\n",
       "      <th>...</th>\n",
       "      <th>Det_opinion_seas_vacc_effective</th>\n",
       "      <th>Det_opinion_h1n1_risk</th>\n",
       "      <th>Det_behavioral_wash_hands</th>\n",
       "      <th>Det_opinion_h1n1_sick_from_vacc</th>\n",
       "      <th>Det_behavioral_face_mask</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>12 Years</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>lzgpxyit</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>kbazzjca</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Male</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Det_income_poverty  Det_doctor_recc_h1n1  \\\n",
       "25194  <= $75,000, Above Poverty                   1.0   \n",
       "14006  <= $75,000, Above Poverty                   0.0   \n",
       "11285  <= $75,000, Above Poverty                   0.0   \n",
       "2900               Below Poverty                   0.0   \n",
       "19083  <= $75,000, Above Poverty                   1.0   \n",
       "\n",
       "       Det_doctor_recc_seasonal Det_rent_or_own Det_employment_status  \\\n",
       "25194                       0.0             Own    Not in Labor Force   \n",
       "14006                       1.0             Own              Employed   \n",
       "11285                       0.0             Own              Employed   \n",
       "2900                        0.0             Own              Employed   \n",
       "19083                       1.0             Own              Employed   \n",
       "\n",
       "          Det_education Det_marital_status  Det_chronic_med_condition  \\\n",
       "25194          12 Years        Not Married                        0.0   \n",
       "14006      Some College            Married                        1.0   \n",
       "11285  College Graduate        Not Married                        0.0   \n",
       "2900   College Graduate        Not Married                        1.0   \n",
       "19083  College Graduate            Married                        0.0   \n",
       "\n",
       "       Det_child_under_6_months  Det_health_worker  ...  \\\n",
       "25194                       0.0                0.0  ...   \n",
       "14006                       1.0                0.0  ...   \n",
       "11285                       0.0                0.0  ...   \n",
       "2900                        0.0                0.0  ...   \n",
       "19083                       0.0                0.0  ...   \n",
       "\n",
       "       Det_opinion_seas_vacc_effective  Det_opinion_h1n1_risk  \\\n",
       "25194                              4.0                    2.0   \n",
       "14006                              4.0                    2.0   \n",
       "11285                              4.0                    1.0   \n",
       "2900                               4.0                    1.0   \n",
       "19083                              1.0                    1.0   \n",
       "\n",
       "       Det_behavioral_wash_hands  Det_opinion_h1n1_sick_from_vacc  \\\n",
       "25194                        0.0                              2.0   \n",
       "14006                        1.0                              1.0   \n",
       "11285                        0.0                              1.0   \n",
       "2900                         0.0                              1.0   \n",
       "19083                        1.0                              2.0   \n",
       "\n",
       "       Det_behavioral_face_mask                census_msa     sex  \\\n",
       "25194                       0.0                   Non-MSA  Female   \n",
       "14006                       0.0  MSA, Not Principle  City  Female   \n",
       "11285                       0.0       MSA, Principle City  Female   \n",
       "2900                        0.0  MSA, Not Principle  City    Male   \n",
       "19083                       0.0  MSA, Not Principle  City  Female   \n",
       "\n",
       "           age_group  hhs_geo_region   race  \n",
       "25194  18 - 34 Years        oxchjgsf  White  \n",
       "14006  45 - 54 Years        lzgpxyit  White  \n",
       "11285  45 - 54 Years        kbazzjca  White  \n",
       "2900   55 - 64 Years        mlyzmhmf  White  \n",
       "19083  18 - 34 Years        bhuqouqj  White  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves X_train_pred to pickle file for ease to reload\n",
    "with open('data/temp_pickle_files/X_train_pred.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens X_train_pred pickle file\n",
    "with open('data/temp_pickle_files/X_train_pred.pickle', 'rb') as f:\n",
    "    X_train_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <a id='h1n1_cdc_article' href='https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html'>https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html</a>\n",
    "\n",
    "[2] <a id='About the National Immunization Survery' href=\"https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1\">https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1</a>\n",
    "\n",
    "[3] <a href='https://www.drivendata.org/competitions/66/flu-shot-learning/data/'>https://www.drivendata.org/competitions/66/flu-shot-learning/data/</a>\n",
    "\n",
    "[4] <a id='Missing Data Imputation using Regression' href='https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression'>https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe.get_feature_names(X_train_imp.drop(missing_columns, axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrote but not forgotten\n",
    "\n",
    "possible add backs later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_feature_info(feature, target):\n",
    "    '''\n",
    "    Prints out dataframe resembling a confusion matrix, with totals included\n",
    "    '''\n",
    "\n",
    "    ones = pd.Series(np.ones(feature.shape[0]), name='filler')\n",
    "    temp_df = pd.concat([target, feature, ones], axis=1).dropna()\n",
    "    total = temp_df.shape[0]\n",
    "    df = pd.crosstab(temp_df[target.name], temp_df[feature.name], normalize=True)\n",
    "    stats.chi2_contingency(df)[1]\n",
    "    column_tot = pd.Series(df.sum(), name='total')\n",
    "    df = df.append(column_tot)\n",
    "    row_tot = pd.Series(df.sum(axis=1), name='total')\n",
    "    df = pd.concat([df, row_tot], axis=1)\n",
    "\n",
    "    \n",
    "    display(df)\n",
    "    print(f'{target.name} vs. {feature.name}: Total of {total} non null combinations.')\n",
    "    print('Chi2 score: {:.5E}'.format(chi2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_of_interest = ['h1n1_concern', \n",
    "#                         'h1n1_knowledge', \n",
    "#                         'doctor_recc_h1n1', \n",
    "#                         'chronic_med_condition', \n",
    "#                         'health_worker', \n",
    "#                         'age_group', \n",
    "#                         'education']\n",
    "\n",
    "# for feature in features_of_interest:\n",
    "#     quick_feature_info(X[feature], y)\n",
    "#     print(f'{feature}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option for second dataframe needs to be fixed, combinations iterates through all columns of dataframe\n",
    "# which is not the intention, need to set up 3 sets and perform combinations\n",
    "# 1. shared features between two inputed dataframes\n",
    "# 2. unique columns features in features DataFrame handled iterativly with target dataframe\n",
    "# 3. Same for unique features in target DataFrame with features dataframe\n",
    "# May be best to create individual function handling one feature with a dataframe as in the pd.series branch of if statement\n",
    "\n",
    "# def ordered_chi2(features, target='features', ascending=True, alpha=None):\n",
    "#     '''\n",
    "#     Returnes the ordered chi2 P-values between each feature and the target variable or variables\n",
    "    \n",
    "#     Inputs: features = Dataframe of features\n",
    "#             target = DataFrame or Series of features or target variable, \n",
    "#                      features with different entries can't share a name\n",
    "#                      default = 'features' which results in finding relationships\n",
    "#                      inside features DataFrame\n",
    "#             ascending = Determines order, default value = True\n",
    "#             alpha = Chi2 Pvalue threshold for which features get returned.\n",
    "#                         Returns all features with a Pvalue<=alpha,\n",
    "#                         default = None so all features are returned\n",
    "            \n",
    "#     Output: Dataframe containing ordered P-values\n",
    "#     '''\n",
    "    \n",
    "#     df = pd.DataFrame(columns=['pair', 'Pvalue'])\n",
    "                \n",
    "#     if isinstance(target, pd.DataFrame):\n",
    "#         feature_cols = set(features.columns)\n",
    "#         target_cols = set(target.columns)\n",
    "#         add_target_cols = target_cols - feature_cols\n",
    "#         col_set = feature_cols.union(add_target_cols)\n",
    "#         temp_df = pd.concat([features, target[add_target_cols]])\n",
    "#         combs = combinations(col_set, 2)\n",
    "        \n",
    "#         for comb in combs:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [comb]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(temp_df[comb[0]], temp_df[comb[1]]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "#     elif isinstance(target, pd.Series):\n",
    "#         for col in features.columns:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [target.name, col]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(target, features[col]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "#     elif target == 'features':\n",
    "#         combs = combinations(features.columns, 2)\n",
    "#         for comb in combs:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [comb]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(features[comb[0]], features[comb[1]]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "        \n",
    "#     else:\n",
    "#         sys.exit('''Incorrect input for parameter target.\n",
    "#         Parameter only accepts types pd.DataFrame, pd.Series, or left to default value.''')        \n",
    "    \n",
    "#     if alpha == None:\n",
    "#         return df.sort_values(by='Pvalue', ascending=ascending)\n",
    "#     else:\n",
    "#         return df[df.Pvalue <= alpha].sort_values(by='Pvalue', ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs numberical columns from training set\n",
    "X_train_num = X_train_mod.select_dtypes(include=['int64', 'float64']).copy()\n",
    "\n",
    "def scatter_plots(X, n_cols, y='index'):\n",
    "    '''\n",
    "    Creates scatter plots for each feature in X against y\n",
    "    Handles missing values of type Nan \n",
    "    \n",
    "    Inputs: X = dataframe to have features plotted\n",
    "            y = dataseries to be plotted against,\n",
    "                default=index\n",
    "            ncols = number of columns for figure\n",
    "    Output: [# of features//ncols, ncols] sized figure of scatterplots\n",
    "    '''\n",
    "#     Calculates number of rows in figure\n",
    "    n_rows = (X.shape[1]//n_cols)+1\n",
    "    \n",
    "#     Creates figure\n",
    "    fig, axes = plt.subplots(figsize = (n_cols*5, n_rows*5),\n",
    "                                  ncols=n_cols,\n",
    "                                  nrows=n_rows);\n",
    "#     Plots individual scatter plots\n",
    "    for i, col in enumerate(X.columns):\n",
    "        if type(y) == str:\n",
    "            ind = range(X[col].notna().sum())\n",
    "            axes[i//n_cols, i%n_cols].scatter(ind, X[col].dropna());\n",
    "            axes[i//n_cols, i%n_cols].set_title(f'{col} vs. {y}');\n",
    "        else:\n",
    "            temp_df = pd.concat([X[col], y], axis=1).dropna()\n",
    "            axes[i//n_cols, i%n_cols].scatter(temp_df.iloc[:,1], temp_df.iloc[:,0]);\n",
    "            axes[i//n_cols, i%n_cols].set_title(f'{col} vs. {y.name}');\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
