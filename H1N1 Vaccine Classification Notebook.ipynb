{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import (LabelEncoder, FunctionTransformer,\n",
    "                                   OneHotEncoder)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import (BaseEstimator, TransformerMixin)\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix)\n",
    "from sklearn.feature_selection import chi2\n",
    "from itertools import combinations\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Pandemics have occured throughout history, each time affecting a large portion of the population. The most recent case being the Covid-19 2020 outbreak which resulted in a prolonged change in peoples day to day life, a lack or resources, and ultimately many deaths all around the world. If possible governments will do their best to try and prevent any future outbreaks by observing and learning from past information.     \n",
    "\n",
    "Therefore our stakeholder, a government agency, is trying to plan for future pandemic prevention and awareness by using the 2009 H1N1 pandemic as an example. They would like us to observe which features of a survey completed at the time appear to hold the highest importance in people who did not recieve the H1N1 vaccine. With this information they are hoping to be able to concentrate their efforts to provide vaccination information and flu prevention methods to a group that was otherwise more succeptible to contracting the virus. These efforts will be performed in hopes of increasing the vaccination rates and to help limit the spread of future viruses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "1. Determine business objectives: You should first “thoroughly understand, from a business perspective, what the customer really wants to accomplish.” (CRISP-DM Guide) and then define business success criteria.\n",
    "2. Assess situation: Determine resources availability, project requirements, assess risks and contingencies, and conduct a cost-benefit analysis.\n",
    "3. Determine data mining goals: In addition to defining the business objectives, you should also define what success looks like from a technical data mining perspective.\n",
    "4. Produce project plan: Select technologies and tools and define detailed plans for each project phase.\n",
    "\n",
    "Our stakeholders goal is to help in the prevention of future pandemics by spreading vaccination and prevention information. To increase the value of their efforts they would like to target the population that are less inclined to receive the vaccination. The most successful outcome would be the prevention of a widespread outbreak resulting in a pandemic, but more realisticly an increase in vaccination awareness that results in a higher rate of people receiving vaccinations in case another outbreak occurs.\n",
    "\n",
    "Therefore this projects requirements are to define the target audience who are less inclined to receive a vaccination. In order to do so a data on people who have both received and refrained from receiving vaccinations in the past is required. Available to us is a survey conducted for the 2009 H1N1 pandemic. The H1N1 outbreak was first detected in the United States and quickly spread across the rest of the world resuliting in between 151,000 and 575,000 deaths worldwide. Unlike prior strains of the H1N1 virus, people under 65 were more affected than the older population. Around 80 percent of the deaths assumed caused by this strain of H1N1 were people under the age of 65. Since this strain differend from previous strains the seasonal flu vaccinations didn't offer protection from the virus causing a late production of a vaccine that would be affective. An affective vaccination didn't get mass produced until after a second wave of the virus had come and gone in the United States <a href=\"#h1n1_cdc_article\">[1]</a>.\n",
    "\n",
    "Due to these factors this dataset may stray from a typical case, reason being: \n",
    "\n",
    "1. <strong>The late emergence of the mass produced vaccine.</strong> It wasn't until after the second outbreak had passed that it was available, possibly causing people to assume the worst was over and a vaccination wasn't required and lowering the number of vaccinations.\n",
    "\n",
    "2. <strong>The age group most affected were people under 65.</strong> This isn't typical for outbreaks and may have caused the number of vaccinations in this age group to be inflated.\n",
    "\n",
    "Though our dataset may not reflect these two points, they should be kept in mind while analyzing the results. As far as metrics to qualify our models performance, I believe accuracy and precision should be used. A result of having high precision will ensure that our feature importance will strongly relate to true positive entries. Though there should be a fine balance and recall shouldn't be completely forgone. While we don't want our predicted positives to contain many false positives, we would like to predict a good portion of out true positive entries. By approaching our problem by utilizing our metrics in this manner we can assure that our stakeholder is targeting the correct audience.\n",
    "\n",
    "\n",
    "<!-- With these targets in mind, our goal for this project is to find the most important features included in this survey to determine a target audience and possibly prevention methods that can be concentrated on to help prevent future pandemics. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "As mentioned earlier in the business understanding section, the data being used for this analysis will be a 2009 survey conducted for the H1N1 outbreak. This survey was performed by the CDC in order to monitor and evaluate the flu vaccination efforts amount adults and children. The participants were parts of randomly called US households. The questions asked the participants dealt with their H1N1 vaccination status, flu-related behaviors, opinions about flue vaccine safety and effectivenss, recent respiratory illness, and pneumococcal vaccination status <a href=\"#About the National Immunization Survery\">[2]</a>.\n",
    "\n",
    "A detailed explanation of the features included in this survey can be found <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1%20and%20Seasonal%20Flu%20Vaccines%20Feature%20Information.txt\">here</a> or <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/\">here</a>.\n",
    "<!--  link need to fitted to github repository when pushed -->\n",
    "\n",
    "The following data from the survey can be found <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/data/\">here</a> <a href=\"#Source Data Download\">[3]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import survey data into dataframes\n",
    "# The source dataset already had this split feature and target files\n",
    "X = pd.read_csv('data/source_data/training_set_features.csv')\n",
    "y = pd.read_csv('data/source_data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training_set_labels.csv` file read to `y` contains two target variables, `h1n1_vaccine` and `seasonal_vaccine1`. For this project only the `h1n1_vaccine` target variable will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets target variable\n",
    "y = y.h1n1_vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21033\n",
      "1     5674\n",
      "Name: h1n1_vaccine, dtype: int64\n",
      "0    0.787546\n",
      "1    0.212454\n",
      "Name: h1n1_vaccine, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checks distribution of target variable\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 79% of those who were surveyed did not receive the vaccination. \n",
    "\n",
    "Let't take a look at the featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>income_poverty</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...             income_poverty  marital_status  \\\n",
       "0                    1.0  ...              Below Poverty     Not Married   \n",
       "1                    1.0  ...              Below Poverty     Not Married   \n",
       "2                    0.0  ...  <= $75,000, Above Poverty     Not Married   \n",
       "3                    0.0  ...              Below Poverty     Not Married   \n",
       "4                    1.0  ...  <= $75,000, Above Poverty         Married   \n",
       "\n",
       "   rent_or_own   employment_status  hhs_geo_region                census_msa  \\\n",
       "0          Own  Not in Labor Force        oxchjgsf                   Non-MSA   \n",
       "1         Rent            Employed        bhuqouqj  MSA, Not Principle  City   \n",
       "2          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "3         Rent  Not in Labor Force        lrircsnp       MSA, Principle City   \n",
       "4          Own            Employed        qufhixun  MSA, Not Principle  City   \n",
       "\n",
       "   household_adults  household_children  employment_industry  \\\n",
       "0               0.0                 0.0                  NaN   \n",
       "1               0.0                 0.0             pxcmvdjn   \n",
       "2               2.0                 0.0             rucpziij   \n",
       "3               0.0                 0.0                  NaN   \n",
       "4               1.0                 0.0             wxleyezf   \n",
       "\n",
       "   employment_occupation  \n",
       "0                    NaN  \n",
       "1               xgwztkwe  \n",
       "2               xtkaffoo  \n",
       "3                    NaN  \n",
       "4               emcorrxb  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview of feature dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the `respondent_id` column, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26707 entries, 0 to 26706\n",
      "Data columns (total 35 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   h1n1_concern                 26615 non-null  float64\n",
      " 1   h1n1_knowledge               26591 non-null  float64\n",
      " 2   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 3   behavioral_avoidance         26499 non-null  float64\n",
      " 4   behavioral_face_mask         26688 non-null  float64\n",
      " 5   behavioral_wash_hands        26665 non-null  float64\n",
      " 6   behavioral_large_gatherings  26620 non-null  float64\n",
      " 7   behavioral_outside_home      26625 non-null  float64\n",
      " 8   behavioral_touch_face        26579 non-null  float64\n",
      " 9   doctor_recc_h1n1             24547 non-null  float64\n",
      " 10  doctor_recc_seasonal         24547 non-null  float64\n",
      " 11  chronic_med_condition        25736 non-null  float64\n",
      " 12  child_under_6_months         25887 non-null  float64\n",
      " 13  health_worker                25903 non-null  float64\n",
      " 14  health_insurance             14433 non-null  float64\n",
      " 15  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 16  opinion_h1n1_risk            26319 non-null  float64\n",
      " 17  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 18  opinion_seas_vacc_effective  26245 non-null  float64\n",
      " 19  opinion_seas_risk            26193 non-null  float64\n",
      " 20  opinion_seas_sick_from_vacc  26170 non-null  float64\n",
      " 21  age_group                    26707 non-null  object \n",
      " 22  education                    25300 non-null  object \n",
      " 23  race                         26707 non-null  object \n",
      " 24  sex                          26707 non-null  object \n",
      " 25  income_poverty               22284 non-null  object \n",
      " 26  marital_status               25299 non-null  object \n",
      " 27  rent_or_own                  24665 non-null  object \n",
      " 28  employment_status            25244 non-null  object \n",
      " 29  hhs_geo_region               26707 non-null  object \n",
      " 30  census_msa                   26707 non-null  object \n",
      " 31  household_adults             26458 non-null  float64\n",
      " 32  household_children           26458 non-null  float64\n",
      " 33  employment_industry          13377 non-null  object \n",
      " 34  employment_occupation        13237 non-null  object \n",
      "dtypes: float64(23), object(12)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# drops respondent_id column\n",
    "X.drop('respondent_id', axis=1, inplace=True)\n",
    "\n",
    "# preview at feature column information\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datasets contains 26,707 entries and 35 features. The features have been inputted as either float64 or object types, 23 and 12 columns of those types respectively. As mentioned in the <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1%20and%20Seasonal%20Flu%20Vaccines%20Feature%20Information.txt\">feature description</a> file, the float64 column types are either encoded or binary where Yes=1 and No=0. It can also be observed from the above information that there are many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h1n1_concern                      92\n",
       "h1n1_knowledge                   116\n",
       "behavioral_antiviral_meds         71\n",
       "behavioral_avoidance             208\n",
       "behavioral_face_mask              19\n",
       "behavioral_wash_hands             42\n",
       "behavioral_large_gatherings       87\n",
       "behavioral_outside_home           82\n",
       "behavioral_touch_face            128\n",
       "doctor_recc_h1n1                2160\n",
       "doctor_recc_seasonal            2160\n",
       "chronic_med_condition            971\n",
       "child_under_6_months             820\n",
       "health_worker                    804\n",
       "health_insurance               12274\n",
       "opinion_h1n1_vacc_effective      391\n",
       "opinion_h1n1_risk                388\n",
       "opinion_h1n1_sick_from_vacc      395\n",
       "opinion_seas_vacc_effective      462\n",
       "opinion_seas_risk                514\n",
       "opinion_seas_sick_from_vacc      537\n",
       "age_group                          0\n",
       "education                       1407\n",
       "race                               0\n",
       "sex                                0\n",
       "income_poverty                  4423\n",
       "marital_status                  1408\n",
       "rent_or_own                     2042\n",
       "employment_status               1463\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "household_adults                 249\n",
       "household_children               249\n",
       "employment_industry            13330\n",
       "employment_occupation          13470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks amount of Nan values in feature dataframe\n",
    "missing_values = X.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all columns are missing values, and some are missing nearly half of their values missing. These columns may be dropped later while the others are filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving columns with many missing entries for later\n",
    "many_missing = missing_values[missing_values > 12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check correlation between our target variable and features. We will be using a Chi2 test, with alpha=.05 and our null hypothesis being that there is no relationship between our feature and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the feature and target variable\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the feature and target variable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_chi2(features, target='features', ascending=True, alpha=None):\n",
    "    '''\n",
    "    Returnes the ordered chi2 P-values between each feature and the target variable or variables\n",
    "    \n",
    "    Inputs: features = Dataframe of features\n",
    "            target = pd.Series as target or default = 'features' which results in \n",
    "            finding relationships inside features DataFrame\n",
    "            ascending = Determines order, default value = True\n",
    "            alpha = Chi2 Pvalue threshold for which features get returned.\n",
    "                        Returns all features with a Pvalue<=alpha,\n",
    "                        default = None so all features are returned\n",
    "            \n",
    "    Output: Dataframe containing ordered P-values\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(columns=['pair', 'Pvalue'])\n",
    "                 \n",
    "    if isinstance(target, pd.Series):\n",
    "        for col in features.columns:\n",
    "            temp_dict={}\n",
    "            temp_dict['pair'] = [target.name, col]\n",
    "            temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(target, features[col]))[1]\n",
    "            df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "    elif target == 'features':\n",
    "        combs = combinations(features.columns, 2)\n",
    "        for comb in combs:\n",
    "            temp_dict={}\n",
    "            temp_dict['pair'] = [comb]\n",
    "            temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(features[comb[0]], features[comb[1]]))[1]\n",
    "            df = df.append(temp_dict, ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        sys.exit('''Incorrect input for parameter target.\n",
    "        Parameter only accepts types pd.DataFrame, pd.Series, or left to default value.''')        \n",
    "    \n",
    "    if alpha == None:\n",
    "        return df.sort_values(by='Pvalue', ascending=ascending)\n",
    "    else:\n",
    "        return df[df.Pvalue <= alpha].sort_values(by='Pvalue', ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>Pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[h1n1_vaccine, opinion_seas_risk]</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[h1n1_vaccine, opinion_h1n1_risk]</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[h1n1_vaccine, opinion_h1n1_vacc_effective]</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[h1n1_vaccine, doctor_recc_h1n1]</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[h1n1_vaccine, doctor_recc_seasonal]</td>\n",
       "      <td>7.176678e-237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[h1n1_vaccine, opinion_seas_vacc_effective]</td>\n",
       "      <td>1.719836e-218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[h1n1_vaccine, employment_industry]</td>\n",
       "      <td>5.786809e-175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[h1n1_vaccine, employment_occupation]</td>\n",
       "      <td>2.021881e-172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[h1n1_vaccine, health_worker]</td>\n",
       "      <td>4.345311e-164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[h1n1_vaccine, h1n1_knowledge]</td>\n",
       "      <td>8.631658e-87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[h1n1_vaccine, h1n1_concern]</td>\n",
       "      <td>1.091885e-86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[h1n1_vaccine, opinion_h1n1_sick_from_vacc]</td>\n",
       "      <td>2.389411e-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[h1n1_vaccine, chronic_med_condition]</td>\n",
       "      <td>1.486270e-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[h1n1_vaccine, health_insurance]</td>\n",
       "      <td>7.934475e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[h1n1_vaccine, behavioral_wash_hands]</td>\n",
       "      <td>3.954447e-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[h1n1_vaccine, behavioral_touch_face]</td>\n",
       "      <td>1.929485e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[h1n1_vaccine, behavioral_face_mask]</td>\n",
       "      <td>1.528632e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[h1n1_vaccine, child_under_6_months]</td>\n",
       "      <td>6.172942e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[h1n1_vaccine, education]</td>\n",
       "      <td>1.706407e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[h1n1_vaccine, income_poverty]</td>\n",
       "      <td>2.711955e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[h1n1_vaccine, marital_status]</td>\n",
       "      <td>4.277018e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[h1n1_vaccine, hhs_geo_region]</td>\n",
       "      <td>5.188916e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[h1n1_vaccine, behavioral_avoidance]</td>\n",
       "      <td>9.454102e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[h1n1_vaccine, age_group]</td>\n",
       "      <td>2.971603e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[h1n1_vaccine, race]</td>\n",
       "      <td>2.555777e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[h1n1_vaccine, behavioral_antiviral_meds]</td>\n",
       "      <td>4.319995e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[h1n1_vaccine, opinion_seas_sick_from_vacc]</td>\n",
       "      <td>1.903307e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[h1n1_vaccine, rent_or_own]</td>\n",
       "      <td>5.892166e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[h1n1_vaccine, household_adults]</td>\n",
       "      <td>1.100713e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[h1n1_vaccine, employment_status]</td>\n",
       "      <td>5.472123e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[h1n1_vaccine, behavioral_outside_home]</td>\n",
       "      <td>4.060957e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[h1n1_vaccine, sex]</td>\n",
       "      <td>7.709155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[h1n1_vaccine, behavioral_large_gatherings]</td>\n",
       "      <td>3.826512e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[h1n1_vaccine, household_children]</td>\n",
       "      <td>4.585274e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[h1n1_vaccine, census_msa]</td>\n",
       "      <td>9.445420e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           pair         Pvalue\n",
       "19            [h1n1_vaccine, opinion_seas_risk]   0.000000e+00\n",
       "16            [h1n1_vaccine, opinion_h1n1_risk]   0.000000e+00\n",
       "15  [h1n1_vaccine, opinion_h1n1_vacc_effective]   0.000000e+00\n",
       "9              [h1n1_vaccine, doctor_recc_h1n1]   0.000000e+00\n",
       "10         [h1n1_vaccine, doctor_recc_seasonal]  7.176678e-237\n",
       "18  [h1n1_vaccine, opinion_seas_vacc_effective]  1.719836e-218\n",
       "33          [h1n1_vaccine, employment_industry]  5.786809e-175\n",
       "34        [h1n1_vaccine, employment_occupation]  2.021881e-172\n",
       "13                [h1n1_vaccine, health_worker]  4.345311e-164\n",
       "1                [h1n1_vaccine, h1n1_knowledge]   8.631658e-87\n",
       "0                  [h1n1_vaccine, h1n1_concern]   1.091885e-86\n",
       "17  [h1n1_vaccine, opinion_h1n1_sick_from_vacc]   2.389411e-54\n",
       "11        [h1n1_vaccine, chronic_med_condition]   1.486270e-52\n",
       "14             [h1n1_vaccine, health_insurance]   7.934475e-48\n",
       "5         [h1n1_vaccine, behavioral_wash_hands]   3.954447e-34\n",
       "8         [h1n1_vaccine, behavioral_touch_face]   1.929485e-31\n",
       "4          [h1n1_vaccine, behavioral_face_mask]   1.528632e-30\n",
       "12         [h1n1_vaccine, child_under_6_months]   6.172942e-27\n",
       "22                    [h1n1_vaccine, education]   1.706407e-26\n",
       "25               [h1n1_vaccine, income_poverty]   2.711955e-17\n",
       "26               [h1n1_vaccine, marital_status]   4.277018e-16\n",
       "29               [h1n1_vaccine, hhs_geo_region]   5.188916e-16\n",
       "3          [h1n1_vaccine, behavioral_avoidance]   9.454102e-15\n",
       "21                    [h1n1_vaccine, age_group]   2.971603e-14\n",
       "23                         [h1n1_vaccine, race]   2.555777e-12\n",
       "2     [h1n1_vaccine, behavioral_antiviral_meds]   4.319995e-11\n",
       "20  [h1n1_vaccine, opinion_seas_sick_from_vacc]   1.903307e-09\n",
       "27                  [h1n1_vaccine, rent_or_own]   5.892166e-08\n",
       "31             [h1n1_vaccine, household_adults]   1.100713e-06\n",
       "28            [h1n1_vaccine, employment_status]   5.472123e-06\n",
       "7       [h1n1_vaccine, behavioral_outside_home]   4.060957e-04\n",
       "24                          [h1n1_vaccine, sex]   7.709155e-04\n",
       "6   [h1n1_vaccine, behavioral_large_gatherings]   3.826512e-03\n",
       "32           [h1n1_vaccine, household_children]   4.585274e-01\n",
       "30                   [h1n1_vaccine, census_msa]   9.445420e-01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_chi2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe tells us that all but two features have failed the null hypothesis (H<sub>0</sub>) and are significant to our target variable. The two features that ended up being insignificant are `household_children` and `census_msa`.\n",
    "\n",
    "The top six features in terms of significance were:\n",
    "\n",
    "- `opinion_seas_risk`\n",
    "- `opinion_h1n1_risk`\n",
    "- `opinion_h1n1_vacc_effective`\n",
    "- `doctor_recc_h1n1`\n",
    "- `doctor_recc_seasonal`\n",
    "- `opinion_seas_vacc_effective`\n",
    "\n",
    "A theme of the top six is that there are basically three topics, asked each about the H1N1 flu and Seasonal flu. The initial assumption is that these pairs may be highly correlated. Let's confirm this by checking the correlation of all features using the Chi2 test again with an alpha of .05. The hypothesis are similar to the previous test between features and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the two features\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the two features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>Pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>[(employment_industry, employment_occupation)]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>[(race, hhs_geo_region)]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>[(health_worker, employment_industry)]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>[(education, employment_occupation)]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>[(education, employment_industry)]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>[(child_under_6_months, opinion_seas_vacc_effe...</td>\n",
       "      <td>0.030892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(behavioral_avoidance, employment_status)]</td>\n",
       "      <td>0.035881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[(behavioral_antiviral_meds, marital_status)]</td>\n",
       "      <td>0.042447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>[(health_worker, marital_status)]</td>\n",
       "      <td>0.044544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>[(doctor_recc_h1n1, race)]</td>\n",
       "      <td>0.047821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  pair    Pvalue\n",
       "594     [(employment_industry, employment_occupation)]  0.000000\n",
       "534                           [(race, hhs_geo_region)]  0.000000\n",
       "383             [(health_worker, employment_industry)]  0.000000\n",
       "528               [(education, employment_occupation)]  0.000000\n",
       "527                 [(education, employment_industry)]  0.000000\n",
       "..                                                 ...       ...\n",
       "347  [(child_under_6_months, opinion_seas_vacc_effe...  0.030892\n",
       "123        [(behavioral_avoidance, employment_status)]  0.035881\n",
       "90       [(behavioral_antiviral_meds, marital_status)]  0.042447\n",
       "376                  [(health_worker, marital_status)]  0.044544\n",
       "283                         [(doctor_recc_h1n1, race)]  0.047821\n",
       "\n",
       "[573 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=.05\n",
    "chi2_df = ordered_chi2(X, alpha=alpha)\n",
    "chi2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the DataFrame above, many features are correlated. There are a total of 573 instances of high correlation between features. Considering we have many missing values to deal with, the high correlation between certain features may help us fill in some missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Now that we have a better understanding of our data, let's prepare it for our models. \n",
    "\n",
    "We'll start with dropping columns and row containing many missing entries. We already found the columns that fit this criteria earlier. While we are performing this all duplicate entries will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of entries: 26707\n",
      "Number of entries after dropped Duplicates: 26707\n"
     ]
    }
   ],
   "source": [
    "X_prep = X.copy()\n",
    "\n",
    "X_prep.drop(many_missing.index, axis=1, inplace=True)\n",
    "\n",
    "# Drops duplicate entries if any exist\n",
    "df_all = pd.concat([X_prep, y], axis=1).drop_duplicates()\n",
    "X_prep = df_all.drop(y.name, axis=1)\n",
    "y_prep = df_all[y.name]\n",
    "\n",
    "print(f'Original number of entries: {X.shape[0]}')\n",
    "print(f'Number of entries after dropped Duplicates: {X_prep.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There weren't any duplicate entries.\n",
    "\n",
    "Before we start editing more values and rows lets create a train-test split to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training and test splits to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y_prep, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how many rows are missing many values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_row_entries(df):\n",
    "    '''\n",
    "    Prints a DataFrame where:\n",
    "        index = num of nan entries in a row\n",
    "        frequency = number of rows with amount of nans defined by index\n",
    "        cum_sum = cumulative sum starting with rows with most missing entries\n",
    "        \n",
    "    inputs: df = DataFrame with missing entries\n",
    "    output: DataFrame, with nan per row information\n",
    "    '''\n",
    "    \n",
    "    # Creates a list with number of missing values in each row\n",
    "    nan_per_row = []\n",
    "    for i in range(df.shape[0]):\n",
    "        nan_per_row.append(df.iloc[i,:].isna().sum())\n",
    "       \n",
    "    # creates dataseries of missing values\n",
    "    nan_per_row_ds = pd.Series(nan_per_row)\n",
    "    nan_per_row_ds.rename('frequency', inplace=True)\n",
    "    \n",
    "    # gets frequeny of nan amounts in rows\n",
    "    nan_row_counts = nan_per_row_ds.value_counts()\n",
    "    \n",
    "#     Orders nan_row_counts descending by most nans down\n",
    "    ordered_missing_nan = nan_row_counts[nan_row_counts.keys().sort_values(ascending=False)]\n",
    "    \n",
    "#     Creates dataseries with cumulative sum\n",
    "    nan_cum_sum = np.cumsum(ordered_missing_nan)   \n",
    "    nan_cum_sum.rename('cum_sum', inplace=True)\n",
    "    \n",
    "#     Creates DataFrame with rows per missing values amount, and cum sum\n",
    "    nan_df = pd.concat([ordered_missing_nan, nan_cum_sum], axis=1)\n",
    "    nan_df.rename_axis('num_of_nans_in_row', inplace=True)\n",
    "    \n",
    "    return nan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>cum_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_of_nans_in_row</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>103</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>90</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>39</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>106</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>288</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>358</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1694</td>\n",
       "      <td>3110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2176</td>\n",
       "      <td>5286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14744</td>\n",
       "      <td>20030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    frequency  cum_sum\n",
       "num_of_nans_in_row                    \n",
       "19                          2        2\n",
       "18                         32       34\n",
       "17                         11       45\n",
       "16                        103      148\n",
       "15                         13      161\n",
       "14                         90      251\n",
       "13                         28      279\n",
       "12                         35      314\n",
       "11                         39      353\n",
       "10                         73      426\n",
       "9                          39      465\n",
       "8                         106      571\n",
       "7                          67      638\n",
       "6                          31      669\n",
       "5                         288      957\n",
       "4                         101     1058\n",
       "3                         358     1416\n",
       "2                        1694     3110\n",
       "1                        2176     5286\n",
       "0                       14744    20030"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find row missing nan information\n",
    "missing_row_entries(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current features dataframe has 32 features after dropping those four columns. That means around 353 entries are missing about a third (11 values) of their features information. Since these entries are missing most of their information lets drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_by_nans(df, y, nan_threshold=None):\n",
    "    '''\n",
    "    Drop rows by quantity of nan values with nan_threshold as cut-off point\n",
    "    \n",
    "    Inputs: \n",
    "            df = DataFrame to be altered, should contain features and target concatenated together\n",
    "            y = target variable column name\n",
    "            nan_threshold = cut-off point to drop rows, default = None which results in a value\n",
    "                            of half or a little larger than number of features\n",
    "    \n",
    "    Outputs: \n",
    "            feature_df = feature DataFrame\n",
    "            y = Target Variable\n",
    "    '''\n",
    "    \n",
    "#     Checks value of nan_threshold\n",
    "    if nan_threshold == None:\n",
    "#         Sets to half of or rounded up from half of feature columns\n",
    "        nan_threshold = math.ceil((len(df.columns)-1)/2)\n",
    "    \n",
    "#     Finds nans contained in each row \n",
    "    nan_per_row = []\n",
    "    for i in range(df.shape[0]):\n",
    "        nan_per_row.append(df.iloc[i,:].isna().sum())\n",
    "    \n",
    "#     Creates temporary column in df for number of nan values\n",
    "    df['nans'] = nan_per_row\n",
    "#     Creates dataframe of rows under nan threshold\n",
    "    df = df[df.nans < nan_threshold]\n",
    "#     Creates target variable\n",
    "    y = df.h1n1_vaccine\n",
    "#     returns tuple (feature_df, target)\n",
    "    return (df.drop(['nans', 'h1n1_vaccine'], axis=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates custom fuction transfromer\n",
    "Many_Nans_Row_Drop_FT = FunctionTransformer(drop_rows_by_nans, kw_args = {'y': 'h1n1_vaccine',\n",
    "                                                                          'nan_threshold': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates training set of both features and target to be transformed\n",
    "train_set = pd.concat([X_train, y_train], axis=1).copy()\n",
    "\n",
    "# Transforms training set\n",
    "X_train_mod, y_train_mod = Many_Nans_Row_Drop_FT.fit_transform(train_set)\n",
    "\n",
    "# Checks change in shape to ensure proper amount of entries were dropped\n",
    "X_train.shape[0] - X_train_mod.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old work, working up to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a random implementation for missing values, in new columns\n",
    "2. iteratively use classificatin models to predict values for missing entries\n",
    "\n",
    "Need to convert objects to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fill in the missing values, a logistic regression model will be used after randomly imputing existing variables but occurence rate for each feature.\n",
    "\n",
    "The process for this is as follows.\n",
    "1. Create copies of each column missing values in dataframe.\n",
    "2. Randomly fill the missing values in these new columns with existing values by frequency.\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_encoder(df):\n",
    "#     '''\n",
    "#     Takes in a dataframe and uses LabelEncoder on non-Nan values in each feature\n",
    "    \n",
    "#     Input: df = DataFrame\n",
    "#     Output: dataframe with Nan values and encoded non-Nan values\n",
    "#     '''\n",
    "    \n",
    "# #     Iterates through each feature and encodes non-Nan values\n",
    "#     for feature in df.columns:\n",
    "#         df.loc[df[feature].notnull(), feature] = LabelEncoder().fit_transform(df.loc[df[feature].notnull(), feature])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatures custom encoder function transformer\n",
    "# Encoder_FT = FunctionTransformer(feature_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grabs object feature column names to be encoded\n",
    "# object_cols = list(X_mod.select_dtypes('object').columns)\n",
    "\n",
    "# # Creates Column Transformer with custom encoder function transformer\n",
    "# Encoder_CT = ColumnTransformer(transformers = [('encoder', Encoder_FT, object_cols)],\n",
    "#                        remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Runs ColumnTransformer on feature dataset\n",
    "# X_mod_encoded = Encoder_CT.fit_transform(X_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates Dataframe using encoded dataset\n",
    "# X_mod_encoded = pd.DataFrame(X_mod_encoded,\n",
    "#                              index = X_mod.index,\n",
    "#                              columns = X_mod.columns)\n",
    "# X_mod_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves X_mod_encoded to pickle file for ease to reload\n",
    "# with open('data/temp_pickle_files/X_mod_encoded.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_mod_encoded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves X_train_mod to pickle file for ease to reload\n",
    "with open('data/temp_pickle_files/X_train_mod.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train_mod, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens X_mod_encoded pickle file\n",
    "# with open('data/temp_pickle_files/X_mod_encoded.pickle', 'rb') as f:\n",
    "#     X_mod_encoded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens X_train_mod pickle file\n",
    "with open('data/temp_pickle_files/X_train_mod.pickle', 'rb') as f:\n",
    "    X_train_mod = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_imputation(df, feature):\n",
    "#     '''\n",
    "#     Fills in missing feature data by randomly imputing existing values by\n",
    "#     occurence rate\n",
    "#     '''\n",
    "    \n",
    "#     number_missing = df[feature].isnull().sum()\n",
    "#     observed_values = df.loc[df[feature].notnull(), feature]\n",
    "#     df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process was modified from an article written by Shashanka Subrahmanya in his article <a href=\"https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression\">Missing Data Imputation using Regression</a>.\n",
    "\n",
    "His process utilized linear regression and didn't create custom transfomers. For the purpose of this project I have modified it to fit my needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RandomImputer(BaseEstimator, TransformerMixin):\n",
    "#     '''\n",
    "#     Randomly imputes values for missing data in new columns.\n",
    "#     Values are based off of existing values and rates of occurences.\n",
    "    \n",
    "#     Initialized with optional parameter columns which mark which\n",
    "#     columns will be transformed, if left to default value 'all', all\n",
    "#     columns with missing values will be filled.\n",
    "#     '''\n",
    "    \n",
    "# #     Initializes class object\n",
    "#     def __init__(self, missing_columns='all_missing_columns'):\n",
    "#         self.missing_columns = missing_columns\n",
    "\n",
    "# #     If columns equals all_missing_columns sets attribute to all missing columns \n",
    "#     def fit(self, X, y=None):\n",
    "#         if self.missing_columns == 'all_missing_columns':\n",
    "#             nan_amount = X.isna().sum()\n",
    "#             self.missing_columns = list(nan_amount[nan_amount>0].index)\n",
    "# #     Handles if single column entered as string\n",
    "#         elif type(self.missing_columns) == str:\n",
    "#             self.missing_columns = [self.missing_columns]\n",
    "#         return self\n",
    "    \n",
    "    \n",
    "#     def transform(self, X, y=None):\n",
    "#         df = X.copy()\n",
    "#         for col in self.missing_columns:\n",
    "# #     Creates copy of column to have calues imputed into\n",
    "#             df[col+'_imp'] = df[col]\n",
    "# #     Finds number of missing values in column\n",
    "#             number_missing = df[col].isnull().sum()\n",
    "# #     Gets existing values from column, repeat values are kept to ensure proper ratio\n",
    "#             observed_values = df.loc[df[col].notnull(), col]\n",
    "# #     Randomly Imputes observed values replacing all missing information\n",
    "#             df.loc[df[col].isnull(), col+'_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n",
    "\n",
    "# #     Creates column index variable to be called to set DataFrame index\n",
    "#         self.features_out = df.columns\n",
    "        \n",
    "#         return df\n",
    "    \n",
    "# #     Returns final columns index\n",
    "#     def get_features_out(self):\n",
    "#         return self.features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.963002\n",
       "False    0.036998\n",
       "Name: employment_status, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mod.employment_status.notnull().value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = X_train_mod.loc[X_train_mod.employment_status.notnull(), 'employment_status'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Employed              0.537918\n",
       "Not in Labor Force    0.404823\n",
       "Unemployed            0.057259\n",
       "Name: employment_status, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employed'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employed', 'Not in Labor Force', 'Unemployed']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5379175682094042"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomImputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Randomly imputes values for missing data in new columns.\n",
    "    Values are based off of existing values and rates of occurences.\n",
    "    \n",
    "    Initialized with optional parameter columns which mark which\n",
    "    columns will be transformed, if left to default value 'all', all\n",
    "    columns with missing values will be filled.\n",
    "    '''\n",
    "    \n",
    "#     Initializes class object\n",
    "    def __init__(self, missing_columns='all_missing_columns'):\n",
    "        self.missing_columns = missing_columns\n",
    "\n",
    "#     If columns equals all_missing_columns sets attribute to all missing columns \n",
    "    def fit(self, X, y=None):\n",
    "        if self.missing_columns == 'all_missing_columns':\n",
    "            nan_amount = X.isna().sum()\n",
    "            self.missing_columns = list(nan_amount[nan_amount>0].index)\n",
    "#     Handles if single column entered as string\n",
    "        elif type(self.missing_columns) == str:\n",
    "            self.missing_columns = [self.missing_columns]\n",
    "        \n",
    "        feature_value_info = {}\n",
    "        for col in self.missing_columns:\n",
    "            feature_value_info[col] = X.loc[X[col].notnull(), col].value_counts(normalize=True)\n",
    "            \n",
    "        self.feature_value_info = feature_value_info\n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        for col in self.missing_columns:\n",
    "#     Creates copy of column to have calues imputed into\n",
    "            df[col+'_imp'] = df[col]\n",
    "#     Finds number of missing values in column\n",
    "            number_missing = df[col].isnull().sum()\n",
    "#     Randomly Imputes observed values replacing all missing information\n",
    "            df.loc[df[col].isnull(), col+'_imp'] = np.random.choice(self.feature_value_info[col].index, \n",
    "                                                                    number_missing, \n",
    "                                                                    replace = True,\n",
    "                                                                    p = self.feature_value_info[col])\n",
    "\n",
    "#     Creates column index variable to be called to set DataFrame index\n",
    "        self.features_out = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "#     Returns final columns index\n",
    "    def get_features_out(self):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates class object\n",
    "rand_imp = RandomImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits and transforms dataset\n",
    "X_train_imp = rand_imp.fit_transform(X_train_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group                          0\n",
       "race                               0\n",
       "sex                                0\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "h1n1_concern_imp                   0\n",
       "h1n1_knowledge_imp                 0\n",
       "behavioral_antiviral_meds_imp      0\n",
       "behavioral_avoidance_imp           0\n",
       "behavioral_face_mask_imp           0\n",
       "behavioral_wash_hands_imp          0\n",
       "behavioral_large_gatherings_imp    0\n",
       "behavioral_outside_home_imp        0\n",
       "behavioral_touch_face_imp          0\n",
       "doctor_recc_h1n1_imp               0\n",
       "doctor_recc_seasonal_imp           0\n",
       "chronic_med_condition_imp          0\n",
       "child_under_6_months_imp           0\n",
       "health_worker_imp                  0\n",
       "opinion_h1n1_vacc_effective_imp    0\n",
       "opinion_h1n1_risk_imp              0\n",
       "opinion_h1n1_sick_from_vacc_imp    0\n",
       "opinion_seas_vacc_effective_imp    0\n",
       "opinion_seas_risk_imp              0\n",
       "opinion_seas_sick_from_vacc_imp    0\n",
       "education_imp                      0\n",
       "income_poverty_imp                 0\n",
       "marital_status_imp                 0\n",
       "rent_or_own_imp                    0\n",
       "employment_status_imp              0\n",
       "household_adults_imp               0\n",
       "household_children_imp             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks if all columns besides original columns with missing data are filled in\n",
    "X_train_imp.drop(rand_imp.missing_columns, axis=1).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each feature has been filled with randomly imputed data, it's time to apply a regression models to predict the values for missing data from the other features data instead of randomly imputing the data.\n",
    "\n",
    "This requires an iterative process for each column containing missing data. Since each column is categorical they will need to be OHE for each iteration and the target variable (column missing data) will need to be treated as a multiclass classification problem. Also since the features contain various amount of information, I would like to iterate through the columns filling by number of missing values. I am not sure if filling in the columns with little data missing first, or a lot of missing data is better. By filling in the columns with less missing data first, it could help achieve more accurate information to fill the columns with many missing entries. On the other hand filling in the columns with many missing values, will take out more randomness for future iterations, by filling in more random columns with predicted values from regression. To start I believe I will try the method of handling columns with many missing entries first. Also since many columns and iterations are needed Decision Trees will used to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>doctor_recc_h1n1</th>\n",
       "      <th>...</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>income_poverty</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Employed</td>\n",
       "      <td>lzgpxyit</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>kbazzjca</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "25194           1.0             1.0                        0.0   \n",
       "14006           2.0             1.0                        0.0   \n",
       "11285           0.0             0.0                        0.0   \n",
       "2900            1.0             1.0                        0.0   \n",
       "19083           2.0             1.0                        1.0   \n",
       "\n",
       "       behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "25194                   0.0                   0.0                    0.0   \n",
       "14006                   1.0                   0.0                    1.0   \n",
       "11285                   0.0                   0.0                    0.0   \n",
       "2900                    0.0                   0.0                    0.0   \n",
       "19083                   1.0                   0.0                    1.0   \n",
       "\n",
       "       behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "25194                          0.0                      0.0   \n",
       "14006                          0.0                      0.0   \n",
       "11285                          0.0                      0.0   \n",
       "2900                           0.0                      0.0   \n",
       "19083                          1.0                      1.0   \n",
       "\n",
       "       behavioral_touch_face  doctor_recc_h1n1  ...   race     sex  \\\n",
       "25194                    0.0               NaN  ...  White  Female   \n",
       "14006                    0.0               0.0  ...  White  Female   \n",
       "11285                    0.0               0.0  ...  White  Female   \n",
       "2900                     0.0               0.0  ...  White    Male   \n",
       "19083                    1.0               1.0  ...  White  Female   \n",
       "\n",
       "                  income_poverty  marital_status  rent_or_own  \\\n",
       "25194                        NaN     Not Married          Own   \n",
       "14006                        NaN         Married          NaN   \n",
       "11285  <= $75,000, Above Poverty     Not Married          Own   \n",
       "2900               Below Poverty     Not Married          Own   \n",
       "19083                        NaN             NaN          NaN   \n",
       "\n",
       "        employment_status  hhs_geo_region                census_msa  \\\n",
       "25194  Not in Labor Force        oxchjgsf                   Non-MSA   \n",
       "14006            Employed        lzgpxyit  MSA, Not Principle  City   \n",
       "11285            Employed        kbazzjca       MSA, Principle City   \n",
       "2900             Employed        mlyzmhmf  MSA, Not Principle  City   \n",
       "19083                 NaN        bhuqouqj  MSA, Not Principle  City   \n",
       "\n",
       "       household_adults  household_children  \n",
       "25194               1.0                 1.0  \n",
       "14006               2.0                 1.0  \n",
       "11285               0.0                 1.0  \n",
       "2900                0.0                 0.0  \n",
       "19083               NaN                 NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35 - 44 Years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19677 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       h1n1_concern  h1n1_knowledge      age_group\n",
       "25194           1.0             1.0  18 - 34 Years\n",
       "14006           2.0             1.0  45 - 54 Years\n",
       "11285           0.0             0.0  45 - 54 Years\n",
       "2900            1.0             1.0  55 - 64 Years\n",
       "19083           2.0             1.0  18 - 34 Years\n",
       "...             ...             ...            ...\n",
       "21575           2.0             1.0  55 - 64 Years\n",
       "5390            1.0             1.0  55 - 64 Years\n",
       "860             2.0             1.0  55 - 64 Years\n",
       "15795           2.0             1.0  35 - 44 Years\n",
       "23654           3.0             1.0  18 - 34 Years\n",
       "\n",
       "[19677 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mod[['h1n1_concern', 'h1n1_knowledge'] + ['age_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIterativeClassification\u001b[39;00m(\u001b[43mBaseEstimator\u001b[49m, TransformerMixin):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, missing_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_missing_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, class_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmany_first\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth\u001b[38;5;241m=\u001b[39mmax_depth\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class IterativeClassification(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_depth=None, missing_columns='all_missing_columns', class_order='many_first'):\n",
    "        self.max_depth=max_depth\n",
    "        self.missing_columns=missing_columns\n",
    "        self.class_order=class_order\n",
    "        \n",
    "#     If columns equals all_missing_columns sets attribute to all missing columns\n",
    "#     missing_columns parameter will be in order of iteration\n",
    "    def fit(self, X, y=None):\n",
    "        if self.missing_columns == 'all_missing_columns':\n",
    "            nan_amount = X.isna().sum()\n",
    "            if self.class_order == 'many_first':\n",
    "                self.missing_columns = list(nan_amount[nan_amount>0].sort_values(ascending=False).index)\n",
    "            elif self.class_order == 'less_first':\n",
    "                self.missing_columns = list(nan_amount[nan_amount>0].sort_values().index)\n",
    "            else:\n",
    "                sys.exit('''Incorrect input for class_order parameter.\n",
    "                    Parameter only accepts values ('many_first', or 'less_first')''')\n",
    "#     Handles if single column entered as string\n",
    "        elif type(self.missing_columns) == str:\n",
    "            self.missing_columns = [self.missing_columns]\n",
    "\n",
    "        leftover_features = list(set(X.columns) - set(self.missing_columns) - \n",
    "                                {col+'_imp' for col in self.missing_columns})\n",
    "        \n",
    "        pred_features = [col+'_imp' for col in self.missing_columns] + leftover_features\n",
    "        pred_df = X[pred_features].copy()\n",
    "#         pred_df = pd.DataFrame(columns = ['Det_'+col for col in self.missing_columns])\n",
    "        models = {}\n",
    "        accuracy_scores = {}\n",
    "        for col in self.missing_columns:\n",
    "#             pred_df['Det_'+col] = X[col+'_imp']\n",
    "            temp_features = list(set(pred_df.columns) - {col+'_imp'})\n",
    "            \n",
    "#             ohe = OneHotEncoder(sparse=False)\n",
    "            ohe = OneHotEncoder()\n",
    "            ohe_features = ohe.fit_transform(pred_df[temp_features])\n",
    "            \n",
    "            dt = DecisionTreeClassifier(max_depth=self.max_depth, random_state=42)\n",
    "            dt.fit(ohe_features, pred_df[col+'_imp'])\n",
    "            \n",
    "            pred_df.loc[X[col].isnull(), col+'_imp'] = dt.predict(ohe_features)[X[col].isnull()]\n",
    "            \n",
    "            models[col] = dt\n",
    "            accuracy_scores[col] = accuracy_score(X.loc[X[col].notnull(), col],\n",
    "                                                dt.predict(ohe_features)[X[col].notnull()])\n",
    "            \n",
    "        self.dt_models = models\n",
    "        self.fit_accuracy_scores = accuracy_scores\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        pred_df = pd.DataFrame(columns = ['Det_'+col for col in self.missing_columns])\n",
    "        accuracy_scores = {}\n",
    "        \n",
    "        for col in self.missing_columns:\n",
    "            pred_df['Det_'+col] = X[col+'_imp']\n",
    "            temp_features = list(set(X.columns) - set(self.missing_columns) - {col+'_imp'})\n",
    "            \n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe_features = ohe.fit_transform(X[temp_features])\n",
    "            \n",
    "            dt_model = self.dt_models[col]\n",
    "            \n",
    "            pred_df.loc[X[col].isnull(), 'Det_'+col] = dt_model.predict(ohe_features)[X[col].isnull()]\n",
    "            \n",
    "            accuracy_scores[col] = accuracy_score(X.loc[X[col].notnull(), col],\n",
    "                                                  dt_model.predict(ohe_features)[X[col].notnull()])\n",
    "        \n",
    "        self.features_predicted = pred_df.columns\n",
    "        self.transformed_accuracy_scores = accuracy_scores\n",
    "        \n",
    "        leftover_features = list(set(X.columns) - set(self.missing_columns) - \n",
    "                                {col+'_imp' for col in self.missing_columns})\n",
    "        \n",
    "        self.features_out = pred_df.columns\n",
    "        \n",
    "        return pd.concat([pred_df, X[leftover_features]], axis=1)\n",
    "    \n",
    "    def get_features_out(self):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IterClass = IterativeClassification(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterativeClassification(max_depth=5,\n",
       "                        missing_columns=['income_poverty', 'doctor_recc_h1n1',\n",
       "                                         'doctor_recc_seasonal', 'rent_or_own',\n",
       "                                         'employment_status', 'education',\n",
       "                                         'marital_status',\n",
       "                                         'chronic_med_condition',\n",
       "                                         'child_under_6_months',\n",
       "                                         'health_worker',\n",
       "                                         'behavioral_avoidance',\n",
       "                                         'behavioral_touch_face',\n",
       "                                         'h1n1_knowledge', 'household_children',\n",
       "                                         'household_adults',\n",
       "                                         'opinion_h1n1_vacc_effective',\n",
       "                                         'behavioral_large_gatherings',\n",
       "                                         'opinion_seas_sick_from_vacc',\n",
       "                                         'h1n1_concern', 'opinion_seas_risk',\n",
       "                                         'behavioral_outside_home',\n",
       "                                         'behavioral_antiviral_meds',\n",
       "                                         'opinion_seas_vacc_effective',\n",
       "                                         'opinion_h1n1_risk',\n",
       "                                         'behavioral_wash_hands',\n",
       "                                         'opinion_h1n1_sick_from_vacc',\n",
       "                                         'behavioral_face_mask'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IterClass.fit(X_train_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'income_poverty': 0.6614163832706879,\n",
       " 'doctor_recc_h1n1': 0.8497936726272353,\n",
       " 'doctor_recc_seasonal': 0.8255295735900963,\n",
       " 'rent_or_own': 0.804622529430824,\n",
       " 'employment_status': 0.7331257586152303,\n",
       " 'education': 0.4684570826750922,\n",
       " 'marital_status': 0.8537805391743892,\n",
       " 'chronic_med_condition': 0.7290803645401823,\n",
       " 'child_under_6_months': 0.9183526383526384,\n",
       " 'health_worker': 0.8887974064735245,\n",
       " 'behavioral_avoidance': 0.7856191744340879,\n",
       " 'behavioral_touch_face': 0.759906045751634,\n",
       " 'h1n1_knowledge': 0.6035723398826232,\n",
       " 'household_children': 0.7320954907161804,\n",
       " 'household_adults': 0.7350540705978372,\n",
       " 'opinion_h1n1_vacc_effective': 0.5806681968885489,\n",
       " 'behavioral_large_gatherings': 0.8137779817449391,\n",
       " 'opinion_seas_sick_from_vacc': 0.5763524193137205,\n",
       " 'h1n1_concern': 0.5066014171381965,\n",
       " 'opinion_seas_risk': 0.5742685289020287,\n",
       " 'behavioral_outside_home': 0.8165137614678899,\n",
       " 'behavioral_antiviral_meds': 0.9531162411455945,\n",
       " 'opinion_seas_vacc_effective': 0.6017410782467036,\n",
       " 'opinion_h1n1_risk': 0.5685126743357426,\n",
       " 'behavioral_wash_hands': 0.855644914995419,\n",
       " 'opinion_h1n1_sick_from_vacc': 0.573993083807974,\n",
       " 'behavioral_face_mask': 0.9323637103336045}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IterClass.fit_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the decision tree classifier didn't work great for all columns, it can definently be said that it performed better than random guessing. Let's proceed with these models to predict our missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = train_IterClass.transform(X_train_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Det_income_poverty</th>\n",
       "      <th>Det_doctor_recc_h1n1</th>\n",
       "      <th>Det_doctor_recc_seasonal</th>\n",
       "      <th>Det_rent_or_own</th>\n",
       "      <th>Det_employment_status</th>\n",
       "      <th>Det_education</th>\n",
       "      <th>Det_marital_status</th>\n",
       "      <th>Det_chronic_med_condition</th>\n",
       "      <th>Det_child_under_6_months</th>\n",
       "      <th>Det_health_worker</th>\n",
       "      <th>...</th>\n",
       "      <th>Det_opinion_seas_vacc_effective</th>\n",
       "      <th>Det_opinion_h1n1_risk</th>\n",
       "      <th>Det_behavioral_wash_hands</th>\n",
       "      <th>Det_opinion_h1n1_sick_from_vacc</th>\n",
       "      <th>Det_behavioral_face_mask</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>race</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>12 Years</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>White</td>\n",
       "      <td>oxchjgsf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Some College</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>White</td>\n",
       "      <td>lzgpxyit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11285</th>\n",
       "      <td>&lt;= $75,000, Above Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>45 - 54 Years</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>White</td>\n",
       "      <td>kbazzjca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>Below Poverty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>College Graduate</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>55 - 64 Years</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>White</td>\n",
       "      <td>mlyzmhmf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>&gt; $75,000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>12 Years</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>18 - 34 Years</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>White</td>\n",
       "      <td>bhuqouqj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Det_income_poverty  Det_doctor_recc_h1n1  \\\n",
       "25194  <= $75,000, Above Poverty                   0.0   \n",
       "14006  <= $75,000, Above Poverty                   0.0   \n",
       "11285  <= $75,000, Above Poverty                   0.0   \n",
       "2900               Below Poverty                   0.0   \n",
       "19083                  > $75,000                   1.0   \n",
       "\n",
       "       Det_doctor_recc_seasonal Det_rent_or_own Det_employment_status  \\\n",
       "25194                       0.0             Own    Not in Labor Force   \n",
       "14006                       1.0             Own              Employed   \n",
       "11285                       0.0             Own              Employed   \n",
       "2900                        0.0             Own              Employed   \n",
       "19083                       1.0             Own              Employed   \n",
       "\n",
       "          Det_education Det_marital_status  Det_chronic_med_condition  \\\n",
       "25194          12 Years        Not Married                        0.0   \n",
       "14006      Some College            Married                        1.0   \n",
       "11285  College Graduate        Not Married                        0.0   \n",
       "2900   College Graduate        Not Married                        1.0   \n",
       "19083          12 Years        Not Married                        0.0   \n",
       "\n",
       "       Det_child_under_6_months  Det_health_worker  ...  \\\n",
       "25194                       0.0                0.0  ...   \n",
       "14006                       1.0                0.0  ...   \n",
       "11285                       0.0                0.0  ...   \n",
       "2900                        0.0                0.0  ...   \n",
       "19083                       0.0                0.0  ...   \n",
       "\n",
       "       Det_opinion_seas_vacc_effective  Det_opinion_h1n1_risk  \\\n",
       "25194                              4.0                    2.0   \n",
       "14006                              4.0                    2.0   \n",
       "11285                              4.0                    1.0   \n",
       "2900                               4.0                    1.0   \n",
       "19083                              1.0                    1.0   \n",
       "\n",
       "       Det_behavioral_wash_hands  Det_opinion_h1n1_sick_from_vacc  \\\n",
       "25194                        0.0                              2.0   \n",
       "14006                        1.0                              1.0   \n",
       "11285                        0.0                              1.0   \n",
       "2900                         0.0                              1.0   \n",
       "19083                        1.0                              2.0   \n",
       "\n",
       "       Det_behavioral_face_mask     sex      age_group  \\\n",
       "25194                       0.0  Female  18 - 34 Years   \n",
       "14006                       0.0  Female  45 - 54 Years   \n",
       "11285                       0.0  Female  45 - 54 Years   \n",
       "2900                        0.0    Male  55 - 64 Years   \n",
       "19083                       0.0  Female  18 - 34 Years   \n",
       "\n",
       "                     census_msa   race  hhs_geo_region  \n",
       "25194                   Non-MSA  White        oxchjgsf  \n",
       "14006  MSA, Not Principle  City  White        lzgpxyit  \n",
       "11285       MSA, Principle City  White        kbazzjca  \n",
       "2900   MSA, Not Principle  City  White        mlyzmhmf  \n",
       "19083  MSA, Not Principle  City  White        bhuqouqj  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves X_train_pred to pickle file for ease to reload\n",
    "with open('data/temp_pickle_files/X_train_pred.pickle', 'wb') as f:\n",
    "    pickle.dump(X_train_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens X_train_pred pickle file\n",
    "with open('data/temp_pickle_files/X_train_pred.pickle', 'rb') as f:\n",
    "    X_train_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <a id='h1n1_cdc_article' href=\"https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html\">https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html</a>\n",
    "\n",
    "[2] <a id='About the National Immunization Survery' href=\"https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1\">https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1</a>\n",
    "\n",
    "[3] <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/data/\">https://www.drivendata.org/competitions/66/flu-shot-learning/data/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age_group_18 - 34 Years', 'age_group_35 - 44 Years',\n",
       "       'age_group_45 - 54 Years', 'age_group_55 - 64 Years',\n",
       "       'age_group_65+ Years', 'race_Black', 'race_Hispanic',\n",
       "       'race_Other or Multiple', 'race_White', 'sex_Female', 'sex_Male',\n",
       "       'hhs_geo_region_atmpeygn', 'hhs_geo_region_bhuqouqj',\n",
       "       'hhs_geo_region_dqpwygqj', 'hhs_geo_region_fpwskwrf',\n",
       "       'hhs_geo_region_kbazzjca', 'hhs_geo_region_lrircsnp',\n",
       "       'hhs_geo_region_lzgpxyit', 'hhs_geo_region_mlyzmhmf',\n",
       "       'hhs_geo_region_oxchjgsf', 'hhs_geo_region_qufhixun',\n",
       "       'census_msa_MSA, Not Principle  City',\n",
       "       'census_msa_MSA, Principle City', 'census_msa_Non-MSA',\n",
       "       'h1n1_concern_imp_0.0', 'h1n1_concern_imp_1.0',\n",
       "       'h1n1_concern_imp_2.0', 'h1n1_concern_imp_3.0',\n",
       "       'h1n1_knowledge_imp_0.0', 'h1n1_knowledge_imp_1.0',\n",
       "       'h1n1_knowledge_imp_2.0', 'behavioral_antiviral_meds_imp_0.0',\n",
       "       'behavioral_antiviral_meds_imp_1.0',\n",
       "       'behavioral_avoidance_imp_0.0', 'behavioral_avoidance_imp_1.0',\n",
       "       'behavioral_face_mask_imp_0.0', 'behavioral_face_mask_imp_1.0',\n",
       "       'behavioral_wash_hands_imp_0.0', 'behavioral_wash_hands_imp_1.0',\n",
       "       'behavioral_large_gatherings_imp_0.0',\n",
       "       'behavioral_large_gatherings_imp_1.0',\n",
       "       'behavioral_outside_home_imp_0.0',\n",
       "       'behavioral_outside_home_imp_1.0', 'behavioral_touch_face_imp_0.0',\n",
       "       'behavioral_touch_face_imp_1.0', 'doctor_recc_h1n1_imp_0.0',\n",
       "       'doctor_recc_h1n1_imp_1.0', 'doctor_recc_seasonal_imp_0.0',\n",
       "       'doctor_recc_seasonal_imp_1.0', 'chronic_med_condition_imp_0.0',\n",
       "       'chronic_med_condition_imp_1.0', 'child_under_6_months_imp_0.0',\n",
       "       'child_under_6_months_imp_1.0', 'health_worker_imp_0.0',\n",
       "       'health_worker_imp_1.0', 'opinion_h1n1_vacc_effective_imp_1.0',\n",
       "       'opinion_h1n1_vacc_effective_imp_2.0',\n",
       "       'opinion_h1n1_vacc_effective_imp_3.0',\n",
       "       'opinion_h1n1_vacc_effective_imp_4.0',\n",
       "       'opinion_h1n1_vacc_effective_imp_5.0', 'opinion_h1n1_risk_imp_1.0',\n",
       "       'opinion_h1n1_risk_imp_2.0', 'opinion_h1n1_risk_imp_3.0',\n",
       "       'opinion_h1n1_risk_imp_4.0', 'opinion_h1n1_risk_imp_5.0',\n",
       "       'opinion_h1n1_sick_from_vacc_imp_1.0',\n",
       "       'opinion_h1n1_sick_from_vacc_imp_2.0',\n",
       "       'opinion_h1n1_sick_from_vacc_imp_3.0',\n",
       "       'opinion_h1n1_sick_from_vacc_imp_4.0',\n",
       "       'opinion_h1n1_sick_from_vacc_imp_5.0',\n",
       "       'opinion_seas_vacc_effective_imp_1.0',\n",
       "       'opinion_seas_vacc_effective_imp_2.0',\n",
       "       'opinion_seas_vacc_effective_imp_3.0',\n",
       "       'opinion_seas_vacc_effective_imp_4.0',\n",
       "       'opinion_seas_vacc_effective_imp_5.0', 'opinion_seas_risk_imp_1.0',\n",
       "       'opinion_seas_risk_imp_2.0', 'opinion_seas_risk_imp_3.0',\n",
       "       'opinion_seas_risk_imp_4.0', 'opinion_seas_risk_imp_5.0',\n",
       "       'opinion_seas_sick_from_vacc_imp_1.0',\n",
       "       'opinion_seas_sick_from_vacc_imp_2.0',\n",
       "       'opinion_seas_sick_from_vacc_imp_3.0',\n",
       "       'opinion_seas_sick_from_vacc_imp_4.0',\n",
       "       'opinion_seas_sick_from_vacc_imp_5.0', 'education_imp_12 Years',\n",
       "       'education_imp_< 12 Years', 'education_imp_College Graduate',\n",
       "       'education_imp_Some College',\n",
       "       'income_poverty_imp_<= $75,000, Above Poverty',\n",
       "       'income_poverty_imp_> $75,000', 'income_poverty_imp_Below Poverty',\n",
       "       'marital_status_imp_Married', 'marital_status_imp_Not Married',\n",
       "       'rent_or_own_imp_Own', 'rent_or_own_imp_Rent',\n",
       "       'employment_status_imp_Employed',\n",
       "       'employment_status_imp_Not in Labor Force',\n",
       "       'employment_status_imp_Unemployed', 'household_adults_imp_0.0',\n",
       "       'household_adults_imp_1.0', 'household_adults_imp_2.0',\n",
       "       'household_adults_imp_3.0', 'household_children_imp_0.0',\n",
       "       'household_children_imp_1.0', 'household_children_imp_2.0',\n",
       "       'household_children_imp_3.0'], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ohe.get_feature_names(X_train_imp.drop(missing_columns, axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrote but not forgotten\n",
    "\n",
    "possible add backs later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_feature_info(feature, target):\n",
    "    '''\n",
    "    Prints out dataframe resembling a confusion matrix, with totals included\n",
    "    '''\n",
    "\n",
    "    ones = pd.Series(np.ones(feature.shape[0]), name='filler')\n",
    "    temp_df = pd.concat([target, feature, ones], axis=1).dropna()\n",
    "    total = temp_df.shape[0]\n",
    "    df = pd.crosstab(temp_df[target.name], temp_df[feature.name], normalize=True)\n",
    "    stats.chi2_contingency(df)[1]\n",
    "    column_tot = pd.Series(df.sum(), name='total')\n",
    "    df = df.append(column_tot)\n",
    "    row_tot = pd.Series(df.sum(axis=1), name='total')\n",
    "    df = pd.concat([df, row_tot], axis=1)\n",
    "\n",
    "    \n",
    "    display(df)\n",
    "    print(f'{target.name} vs. {feature.name}: Total of {total} non null combinations.')\n",
    "    print('Chi2 score: {:.5E}'.format(chi2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_of_interest = ['h1n1_concern', \n",
    "#                         'h1n1_knowledge', \n",
    "#                         'doctor_recc_h1n1', \n",
    "#                         'chronic_med_condition', \n",
    "#                         'health_worker', \n",
    "#                         'age_group', \n",
    "#                         'education']\n",
    "\n",
    "# for feature in features_of_interest:\n",
    "#     quick_feature_info(X[feature], y)\n",
    "#     print(f'{feature}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option for second dataframe needs to be fixed, combinations iterates through all columns of dataframe\n",
    "# which is not the intention, need to set up 3 sets and perform combinations\n",
    "# 1. shared features between two inputed dataframes\n",
    "# 2. unique columns features in features DataFrame handled iterativly with target dataframe\n",
    "# 3. Same for unique features in target DataFrame with features dataframe\n",
    "# May be best to create individual function handling one feature with a dataframe as in the pd.series branch of if statement\n",
    "\n",
    "# def ordered_chi2(features, target='features', ascending=True, alpha=None):\n",
    "#     '''\n",
    "#     Returnes the ordered chi2 P-values between each feature and the target variable or variables\n",
    "    \n",
    "#     Inputs: features = Dataframe of features\n",
    "#             target = DataFrame or Series of features or target variable, \n",
    "#                      features with different entries can't share a name\n",
    "#                      default = 'features' which results in finding relationships\n",
    "#                      inside features DataFrame\n",
    "#             ascending = Determines order, default value = True\n",
    "#             alpha = Chi2 Pvalue threshold for which features get returned.\n",
    "#                         Returns all features with a Pvalue<=alpha,\n",
    "#                         default = None so all features are returned\n",
    "            \n",
    "#     Output: Dataframe containing ordered P-values\n",
    "#     '''\n",
    "    \n",
    "#     df = pd.DataFrame(columns=['pair', 'Pvalue'])\n",
    "                \n",
    "#     if isinstance(target, pd.DataFrame):\n",
    "#         feature_cols = set(features.columns)\n",
    "#         target_cols = set(target.columns)\n",
    "#         add_target_cols = target_cols - feature_cols\n",
    "#         col_set = feature_cols.union(add_target_cols)\n",
    "#         temp_df = pd.concat([features, target[add_target_cols]])\n",
    "#         combs = combinations(col_set, 2)\n",
    "        \n",
    "#         for comb in combs:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [comb]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(temp_df[comb[0]], temp_df[comb[1]]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "#     elif isinstance(target, pd.Series):\n",
    "#         for col in features.columns:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [target.name, col]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(target, features[col]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "    \n",
    "#     elif target == 'features':\n",
    "#         combs = combinations(features.columns, 2)\n",
    "#         for comb in combs:\n",
    "#             temp_dict={}\n",
    "#             temp_dict['pair'] = [comb]\n",
    "#             temp_dict['Pvalue'] = stats.chi2_contingency(pd.crosstab(features[comb[0]], features[comb[1]]))[1]\n",
    "#             df = df.append(temp_dict, ignore_index=True)\n",
    "        \n",
    "#     else:\n",
    "#         sys.exit('''Incorrect input for parameter target.\n",
    "#         Parameter only accepts types pd.DataFrame, pd.Series, or left to default value.''')        \n",
    "    \n",
    "#     if alpha == None:\n",
    "#         return df.sort_values(by='Pvalue', ascending=ascending)\n",
    "#     else:\n",
    "#         return df[df.Pvalue <= alpha].sort_values(by='Pvalue', ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs numberical columns from training set\n",
    "X_train_num = X_train_mod.select_dtypes(include=['int64', 'float64']).copy()\n",
    "\n",
    "def scatter_plots(X, n_cols, y='index'):\n",
    "    '''\n",
    "    Creates scatter plots for each feature in X against y\n",
    "    Handles missing values of type Nan \n",
    "    \n",
    "    Inputs: X = dataframe to have features plotted\n",
    "            y = dataseries to be plotted against,\n",
    "                default=index\n",
    "            ncols = number of columns for figure\n",
    "    Output: [# of features//ncols, ncols] sized figure of scatterplots\n",
    "    '''\n",
    "#     Calculates number of rows in figure\n",
    "    n_rows = (X.shape[1]//n_cols)+1\n",
    "    \n",
    "#     Creates figure\n",
    "    fig, axes = plt.subplots(figsize = (n_cols*5, n_rows*5),\n",
    "                                  ncols=n_cols,\n",
    "                                  nrows=n_rows);\n",
    "#     Plots individual scatter plots\n",
    "    for i, col in enumerate(X.columns):\n",
    "        if type(y) == str:\n",
    "            ind = range(X[col].notna().sum())\n",
    "            axes[i//n_cols, i%n_cols].scatter(ind, X[col].dropna());\n",
    "            axes[i//n_cols, i%n_cols].set_title(f'{col} vs. {y}');\n",
    "        else:\n",
    "            temp_df = pd.concat([X[col], y], axis=1).dropna()\n",
    "            axes[i//n_cols, i%n_cols].scatter(temp_df.iloc[:,1], temp_df.iloc[:,0]);\n",
    "            axes[i//n_cols, i%n_cols].set_title(f'{col} vs. {y.name}');\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
