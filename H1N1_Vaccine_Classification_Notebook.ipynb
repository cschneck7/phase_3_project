{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaccination Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vaccination Bottles](images/vaccine_bottles_shrunk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Carl Schneck<br>\n",
    "**Program**: Data Science Flex<br>\n",
    "**Phase 3 Project**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Pandemics have occured throughout history, each time affecting a large portion of the population. The most recent case being the Covid-19 2020 outbreak which resulted in a prolonged change in peoples day to day life, a lack or resources, and ultimately many deaths all around the world. If possible governments will do their best to try and prevent any future outbreaks by observing and learning from past information.     \n",
    "\n",
    "Therefore our stakeholder, a government agency, is trying to plan for future pandemic prevention and awareness by using the 2009 H1N1 pandemic as an example. They would like us to observe which features of a survey completed at the time appear hold the highest importance in people decisions to recieve the H1N1 vaccine. With this information they are hoping to be able to concentrate their efforts to provide vaccination information and flu prevention methods to a group that was otherwise more succeptible to contracting the virus. These efforts will be performed in hopes of increasing the vaccination rates and to help limit the spread of future viruses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Our stakeholders goal is to help in the prevention of future pandemics by spreading vaccination and prevention information. To increase the value of their efforts they would like to target the population that are less inclined to receive the vaccination. The most successful outcome would be the prevention of a widespread outbreak resulting in a pandemic, but more realisticly an increase in vaccination awareness that results in a higher rate of people receiving vaccinations in case another outbreak occurs.\n",
    "\n",
    "Therefore this projects requirements are to define the target audience who are less inclined to receive a vaccination. In order to do so a dataset on people who have both received and refrained from receiving vaccinations in the past is required. Available to us is a survey conducted for the 2009 H1N1 pandemic. The H1N1 outbreak was first detected in the United States and quickly spread across the rest of the world resuliting in between 151,000 and 575,000 deaths worldwide. Unlike prior strains of the H1N1 virus, people under 65 were more affected than the older population. Around 80 percent of the deaths assumed caused by this strain of H1N1 were people under the age of 65. Since this strain differed from previous strains the seasonal flu vaccinations didn't offer protection from the virus causing a late production of a vaccine that would be affective. An affective vaccination didn't get mass produced until after a second wave of the virus had come and gone in the United States <a href=\"#h1n1_cdc_article\">[1]</a>.\n",
    "\n",
    "Due to these factors this dataset may stray from a typical case, reason being: \n",
    "\n",
    "1. <strong>The late emergence of the mass produced vaccine.</strong> It wasn't until after the second outbreak had passed that it was available, possibly causing people to assume the worst was over and a vaccination wasn't required and lowering the number of vaccinations.\n",
    "\n",
    "2. <strong>The age group most affected were people under 65.</strong> This isn't typical for outbreaks and may have caused the number of vaccinations in this age group to be inflated.\n",
    "\n",
    "Though our dataset may not reflect these two points, they should be kept in mind while analyzing the results. As far as metrics to qualify our models performance, I believe accuracy and precision should be used. A result of having high precision will ensure that our feature importance will strongly relate to true positive entries. Though there should be a fine balance and recall shouldn't be completely forgone. While we don't want our predicted positives to contain many false positives, we would like to predict a good portion of our true positive entries. Approaching our problem by utilizing our metrics in this manner can assure that our stakeholder is targeting the correct audience.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import python_code.my_functions as myfunc\n",
    "import python_code.my_classes as mycl\n",
    "import python_code.pres_figures as pfigs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import (LabelEncoder, FunctionTransformer,\n",
    "                                   OneHotEncoder, OrdinalEncoder,\n",
    "                                   MinMaxScaler)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, cross_validate)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import (BaseEstimator, TransformerMixin)\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             classification_report, precision_score,\n",
    "                             make_scorer)\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import combinations\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "As mentioned earlier in the business understanding section, the data used for this analysis will be a 2009 survey conducted for the H1N1 outbreak. This survey was performed by the CDC in order to monitor and evaluate the flu vaccination efforts of adults and children in randomly selected US households. The questions asked of the participants dealt with their H1N1 vaccination status, flu-related behaviors, opinions about flu vaccine safety and effectivenss, recent respiratory illness, and pneumococcal vaccination status <a href=\"#About the National Immunization Survery\">[2]</a>.\n",
    "\n",
    "A detailed explanation of the features included in this survey can be found <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1_and_Seasonal_Flu_Vaccines_Feature_Information.txt\">here</a> or <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/\">here</a>.\n",
    "<!--  link need to fitted to github repository when pushed -->\n",
    "\n",
    "The following data from the survey can be found and downloaded <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/data/\">here</a> <a href=\"#Source Data Download\">[3]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import survey data into dataframes\n",
    "# The source dataset already had this split feature and target files\n",
    "X = pd.read_csv('data/source_data/training_set_features.csv')\n",
    "y = pd.read_csv('data/source_data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training_set_labels.csv` file read to `y` contains two target variables, `h1n1_vaccine` and `seasonal_vaccine1`. For this project only the `h1n1_vaccine` target variable will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets target variable\n",
    "y = y.h1n1_vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks distribution of target variable\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 79% of those who were surveyed did not receive the vaccination. \n",
    "\n",
    "Let't take a look at the featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of feature dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the `respondent_id` column, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops respondent_id column\n",
    "X.drop('respondent_id', axis=1, inplace=True)\n",
    "\n",
    "# preview at feature column information\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This datasets contains 26,707 entries and 35 features. The features have been inputted as either float64 or object types, 23 and 12 columns of those types respectively. As mentioned in the <a href=\"https://github.com/cschneck7/phase_3_project/blob/main/data/H1N1_and_Seasonal_Flu_Vaccines_Feature_Information.txt\">feature description</a> file, the float64 column types are either encoded or binary where Yes=1 and No=0. It can also be observed from the above information that there are many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks amount of Nan values in feature dataframe\n",
    "missing_values = X.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all columns are missing values, and some are missing nearly half of their values missing. These columns may be dropped later while the others are filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving columns with many missing entries for later\n",
    "many_missing = missing_values[missing_values > 12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at some of the features then appear to stand out as possible having an elevated importance to our target variable. As mentioned before `age_group` is an important metric in this study due to the unique trait of the H1N1 virus affecting different age groups than normal flu viruses. A few other features that may be interesting to have a further look at include:\n",
    "\n",
    "- `doctor_recc_h1n1` - Whether H1N1 flu vaccine was recommended by their doctor\n",
    "- `opinion_h1n1_vacc_effective` - Respondent's opinion about H1N1 vaccine effectiveness.\n",
    "- `opinion_h1n1_risk` - Respondent's opinion about risk of getting sick with H1N1 flu without vaccine.\n",
    "- `income_poverty` - Household annual income of respondent with respect to 2008 Census poverty thresholds.\n",
    "- `race` - Race of respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `age_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by age group\n",
    "myfunc.vacc_rates(X.age_group, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earler the H1N1 virus affected those under 65 more than those above that age, differing from the stereotypical flu. Looking at the grouping above those in the age group of 55-64 had the highest percentage of vaccinations, with 75.7 of people not vaccinated. Those younger than 55 were unvaccinated at a rate of around 80.5% while those above at 77%. I wouldn't say there is conclusive evidence from this chart that the uncharacteristic affect on the those under 65 had an affect on the vaccination rate in those age groups.\n",
    "\n",
    "#### `doctor_recc_h1n1`\n",
    "\n",
    "- H1N1 flu vaccine was recommended by their doctor.\n",
    "\n",
    "No = 0 <br>\n",
    "Yes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by whether a doctor reccomended the H1N1 vaccination\n",
    "myfunc.vacc_rates(X.doctor_recc_h1n1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a clear importance in regards to vaccination status depending on doctor reccomendation. Of those that were recommended the vaccination only 46.7% of them didn't get the vaccine, while 86.3% of those who weren't recommended the vaccine didn't get the vaccine. \n",
    "\n",
    "#### `opinion_h1n1_vacc_effective`\n",
    "\n",
    "- Respondent's opinion about H1N1 vaccine effectiveness\n",
    "\n",
    "1 = Not at all Effective<br>\n",
    "2 = Not very Effective<br>\n",
    "3 = Don't Know<br>\n",
    "4 = Somewhat Effective<br>\n",
    "5 = Very Effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by opinion of H1N1 vaccination effectiveness\n",
    "myfunc.vacc_rates(X.opinion_h1n1_vacc_effective, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear correlation with the opinion of vaccine effectiveness and vaccination rate. As the opinion of effectiveness got higher the the vaccination rate drastically increased.\n",
    "\n",
    "#### `opinion_h1n1_risk`\n",
    "\n",
    "- Respondent's opinion about risk of getting sick with H1N1 flu without vaccine\n",
    "\n",
    "1 = Very Low<br>\n",
    "2 = Somewhat Low<br>\n",
    "3 = Don't Know<br>\n",
    "4 = Somewhat High<br>\n",
    "5 = Very High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by opinion of H1N1 risk \n",
    "myfunc.vacc_rates(X.opinion_h1n1_risk, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to `opinion_h1n1_vacc_effective`, the rate of vaccinations increased as the opinion of risk got higher. Though there appears to be a much smaller group who consider the H1N1 flu's risk to be high in comparison to the group who consider the vaccine to be effective.\n",
    "\n",
    "#### `income_poverty`\n",
    "\n",
    "- Household annual income of respondent with respect to 2008 Census poverty thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by household income\n",
    "myfunc.vacc_rates(X.income_poverty, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate of vaccination appears to be correlated to household income, with the percentage increasing with household income. This could possibly be due to lower income households not having access to health care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick rates on those without health insurance split by household income groupings\n",
    "myfunc.vacc_rates(X.income_poverty, X.health_insurance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows those who don't have insurance in those groupings. There is a clear correlation between income and percentage of people without insurance with 32.6% of those below poverty without insurance, while only 3% those with above \\$75k household income are without insurance.\n",
    "\n",
    "#### `race`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds unvaccinated group rates by respondent's race\n",
    "myfunc.vacc_rates(X.race, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vaccination rates appear to be similar for all races besides `black` where the vaccination rate is about 6% lower.\n",
    "\n",
    "### Feature Correlation Using Chi2\n",
    "\n",
    "Now lets check correlation between our target variable and features. We will be using a Chi2 test, with alpha=.05 and our null hypothesis being that there is no relationship between our feature and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the feature and target variable\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the feature and target variable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc.ordered_chi2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe tells us that all but two features have failed the null hypothesis (H<sub>0</sub>) and are significant to our target variable. The two features that ended up being insignificant are `household_children` and `census_msa`.\n",
    "\n",
    "The top six features in terms of significance were:\n",
    "\n",
    "- `opinion_seas_risk`\n",
    "- `opinion_h1n1_risk`\n",
    "- `opinion_h1n1_vacc_effective`\n",
    "- `doctor_recc_h1n1`\n",
    "- `doctor_recc_seasonal`\n",
    "- `opinion_seas_vacc_effective`\n",
    "\n",
    "A theme of the top six is that there are basically three topics, asked each about the H1N1 flu and Seasonal flu. The initial assumption is that these pairs may be highly correlated. Let's confirm this by checking the correlation of all features using the Chi2 test again with an alpha of .05. The hypothesis are similar to the previous test between features and target variable.\n",
    "\n",
    "Hypothesis:<br><br>\n",
    "&emsp;&emsp;<strong>H<sub>0</sub></strong>: No relationship between the two features\n",
    "<br>\n",
    "&emsp;&emsp;<strong>H<sub>1</sub></strong>: There is a relationship between the two features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=.05\n",
    "chi2_df = myfunc.ordered_chi2(X, alpha=alpha)\n",
    "chi2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the DataFrame above, many features are correlated. There are a total of 573 instances of high correlation between features. Considering we have many missing values to deal with, the high correlation between certain features may help us fill in some missing data. Let's check if our assumption about the questions related to both the H1N1 and Seasonal flu having high multicollinearty is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_features = [['doctor_recc_h1n1', 'doctor_recc_seasonal'],\n",
    "                    ['opinion_h1n1_vacc_effective', 'opinion_seas_vacc_effective'],\n",
    "                    ['opinion_h1n1_risk', 'opinion_seas_risk'],\n",
    "                    ['opinion_h1n1_sick_from_vacc', 'opinion_seas_sick_from_vacc']]\n",
    "\n",
    "for pair in similar_features:\n",
    "    pvalue = \"{:e}\".format(float(chi2_df.loc[((chi2_df.var1 == pair[0]) & (chi2_df.var2 == pair[1])) |\n",
    "                ((chi2_df.var2 == pair[0]) & (chi2_df.var1 == pair[1])), 'Pvalue']))\n",
    "    print(f'({pair[0]}, {pair[1]}) : {pvalue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above our assumptions were correct, for all cases we can reject the null hypothesis of the features having no relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Now that we have a better understanding of our data, let's prepare it for our models. \n",
    "\n",
    "We'll start with dropping columns and row containing many missing entries. We already found the columns that fit this criteria earlier. While we are performing this all duplicate entries will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = X.copy()\n",
    "\n",
    "X_prep.drop(many_missing.index, axis=1, inplace=True)\n",
    "\n",
    "# Drops duplicate entries if any exist\n",
    "df_all = pd.concat([X_prep, y], axis=1).drop_duplicates()\n",
    "X_prep = df_all.drop(y.name, axis=1)\n",
    "y_prep = df_all[y.name]\n",
    "\n",
    "print(f'Original number of entries: {X.shape[0]}')\n",
    "print(f'Number of entries after dropped Duplicates: {X_prep.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There weren't any duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start editing more values and rows lets create a train-test split to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training and test splits to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y_prep, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check how many rows are missing many values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find row missing nan information\n",
    "myfunc.missing_row_entries(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current features dataframe has 32 features after dropping those four columns. That means around 353 entries are missing about a third (11 values) of their features information. Since these entries are missing most of their information lets drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates custom fuction transfromer\n",
    "Many_Nans_Row_Drop_FT = FunctionTransformer(myfunc.drop_rows_by_nans, kw_args = {'y': 'h1n1_vaccine',\n",
    "                                                                          'nan_threshold': 11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates combined set of both features and target to be transformed\n",
    "train_df = pd.concat([X_train, y_train], axis=1).copy()\n",
    "\n",
    "# Transforms training set\n",
    "X_train_mod, y_train_mod = Many_Nans_Row_Drop_FT.fit_transform(train_df)\n",
    "\n",
    "# Checks change in shape to ensure proper amount of entries were dropped\n",
    "X_train.shape[0] - X_train_mod.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our object columns now need to be encoded for later use in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe of object types\n",
    "X_train_mod_obj = X_train_mod.select_dtypes(include='object').copy()\n",
    "\n",
    "# Creates dataframe of numerical features\n",
    "X_train_mod_num = X_train_mod.select_dtypes(exclude='object').copy()\n",
    "\n",
    "# Initialized ordinal encoder object\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "# Creates dataframe of ordinal encoded data\n",
    "X_train_mod_obj = pd.DataFrame(data = oe.fit_transform(X_train_mod_obj),\n",
    "                                columns=X_train_mod_obj.columns,\n",
    "                                index=X_train_mod_obj.index)\n",
    "\n",
    "# Checks newly encoded dataframe\n",
    "X_train_mod_obj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.concat([X_train_mod_obj, X_train_mod_num], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try and fill in the rest of our missing values. Earlier we noticed that many variables had high colinearity with each other, we can use this to help us fill in for missing values. Our first attempt at filling the values will be through classification using the other features.\n",
    "\n",
    "Issue with this process is the existence of missing values, many models in scikit-learn libraries cannot handle Nan values. Therefore random imputation will be used to fill in the feature information. The values will be randomly selected using existing values and their rate of appearances in the features existing values. After that is completed an iterative Decision Tree Classifier will be used to fill the randomly imputed values with predicted values.\n",
    "\n",
    "This process was modified from Shashanka Subrahmanya's process for a regression model in his article <a href=\"https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression\">Missing Data Imputation using Regression</a> <a href='#Missing Data Imputation using Regression'>[4]</a>. His process utilized linear regression and didn't create custom transfomers. For the purpose of this project I have modified it to fit my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will create a RandomImputer object, fit and transform it to our training feature dataset, and then check if all the missing values have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creates class object\n",
    "rand_imp = mycl.RandomImputer()\n",
    "\n",
    "# Fits and transforms dataset\n",
    "X_train_imp = rand_imp.fit_transform(X_train_enc)\n",
    "\n",
    "# checks if all columns besides original columns with missing data are filled in\n",
    "X_train_imp.drop(rand_imp.missing_columns, axis=1).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each feature has been filled with randomly imputed data, it's time to apply a regression models to predict the values for missing data from the other features data instead of randomly imputing the data.\n",
    "\n",
    "This requires an iterative process for each feature that was originally missing data. Since each column is categorical they will need to be OHE for each iteration and the target variable (column missing data) will need to be treated as a multiclass classification problem. Also since the features contain various amount of information, I would like to iterate through the columns filling by number of missing values. I am not sure if filling in the columns with little data missing first, or a lot of missing data is better. By filling in the columns with less missing data first, it could help achieve more accurate information to fill the columns with many missing entries. On the other hand filling in the columns with many missing values, will take out more randomness for future iterations, by filling in more random values with predicted values from classification. To start I believe I will try the method of handling columns with many missing entries first. Also since many columns and iterations are needed, Decision Trees will be used to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IterativeClassification class uses a OneHotEncoding transformer to help fit the data, therefore the categorical and numerical columns must be supplied.\n",
    "\n",
    "The features to be considered categorical will be those with more than 2 unique values, aren't opinion based, or age. The opinion based features like `h1n1_concern` have values from 1-5 that are in order of magnitude.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all features that are opinion based with entries ranked from 1-5\n",
    "opinion_cols = [col for col in X_train_mod.columns if ('opinion' in col)] + ['h1n1_concern', 'h1n1_knowledge']\n",
    "\n",
    "# Finds features that have more than 2 unique values\n",
    "many_unique_cols = X_train_mod.nunique() > 2\n",
    "many_unique = many_unique_cols.index[many_unique_cols]\n",
    "\n",
    "# Subtracts opinion features from list with features with greater than 2 unique values\n",
    "# to get categorical columns for one hot encoding\n",
    "cat_cols = list(set(many_unique) - set(opinion_cols) - {'age_group'})\n",
    "\n",
    "# Previews left over categorical columns\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates list of numerical features\n",
    "num_cols = list(set(X_train_mod.columns) - set(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell the IterativeClassification class object is created, fit and the accuracy scores are checked. The accuracy score was checked with each iteration with the first entry being the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creates IterativeClassification class object with max_depth = 5\n",
    "iter_class = mycl.IterativeClassification(max_depth=5, cat_cols=cat_cols, num_cols=num_cols)\n",
    "\n",
    "# Fits object to training data\n",
    "iter_class.fit(X_train_imp)\n",
    "\n",
    "# Checks accuracy model by checking predictions against existing values\n",
    "iter_class.fit_accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the decision tree classifier didn't work great for all columns, it can definently be said that it performed better than random guessing. Let's transform our training set to fill in the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms dataset to fill in missing values\n",
    "X_train_trans = iter_class.transform(X_train_imp)\n",
    "\n",
    "# print first 5 entries of transformed dataset\n",
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_class.pred_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves X_train_trans to pickle file for ease to reload\n",
    "# with open('data/temp_pickle_files/X_train_trans.pickle', 'wb') as f:\n",
    "#     pickle.dump(X_train_trans, f)\n",
    "\n",
    "# Opens X_train_trans pickle file\n",
    "with open('data/temp_pickle_files/X_train_trans.pickle', 'rb') as f:\n",
    "    X_train_trans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now has no missing values. The features with `Det_` starting their names, are those that have been transformed to have their missing values filled in.\n",
    "\n",
    "Now that we've filled in our missing values, let's drop some features that have clear multicollinearity, and poor correlation with our target variable. \n",
    "\n",
    "The features with high multicollinearity that will be dropped were those about the seasonal flu that had a similar question related to the H1N1 flu. These features included:\n",
    "\n",
    "- `doctor_recc_seasonal`\n",
    "- `opinion_seas_vacc_effective`\n",
    "- `opinion_seas_risk`\n",
    "- `opinion_seas_sick_from_vacc`\n",
    "\n",
    "Also found earlier those with poor relationships to our target variable:\n",
    "\n",
    "- `household_children`\n",
    "- `and census_msa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original column names that need to be dropped\n",
    "orig_cols = ['doctor_recc_seasonal', \n",
    "             'opinion_seas_vacc_effective', \n",
    "             'opinion_seas_risk', \n",
    "             'opinion_seas_sick_from_vacc', \n",
    "             'household_children', \n",
    "             'census_msa']\n",
    "\n",
    "# finds current column names in transformed dataframe\n",
    "drop_cols = [col for col in X_train_trans.columns if any(orig_col in col for orig_col in orig_cols)]\n",
    "\n",
    "# Drops columns with poor relationship and multicollinearity\n",
    "X_train_mod2 = X_train_trans.drop(drop_cols, axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have no missing values, and have performed some feature selection, we need to One Hot Encode all our categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets feature names of those to be one hot encoded\n",
    "ohe_columns = [col for col in X_train_mod2.columns if any(cat_col in col for cat_col in cat_cols)]\n",
    "\n",
    "# creates column transformer object with OHE\n",
    "ct_ohe = ColumnTransformer([('ohe', OneHotEncoder(sparse=False), ohe_columns)],\n",
    "                           remainder='passthrough')\n",
    "\n",
    "# fits and transforms column transformer to X_train_mod2\n",
    "X_train_ohe_array = ct_ohe.fit_transform(X_train_mod2)\n",
    "\n",
    "# Get new column names\n",
    "ct_col_names = ct_ohe.get_feature_names_out()\n",
    "\n",
    "# Remove transformer name and decimals from columns names\n",
    "ct_col_names = [col.replace('ohe__', '').replace('remainder__', '').replace('.0', '') for col in ct_col_names]\n",
    "\n",
    "# Creates dataframe with newly encoded data\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe_array,\n",
    "                           columns = ct_col_names,\n",
    "                           index=X_train_mod2.index)\n",
    "\n",
    "# Previews first few entries of data\n",
    "X_train_ohe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is scaling our data. Most of our features have ranges of [0,1], but others are ranged from 1 to 5. This may cause bias in our models, so we will use a MinMaxScaler to adjust our scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fits scaler object and transforms X_train_ohe and creates a dataframe\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_ohe),\n",
    "                              columns=X_train_ohe.columns,\n",
    "                              index=X_train_ohe.index)\n",
    "\n",
    "# previews dataframe\n",
    "X_train_scaled.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that our training features are ready to be used in a model.\n",
    "\n",
    "Let's now bring our test sets up to date with the transformations. The transformations performed on our training set included:\n",
    "\n",
    "1. Dropping rows with many values\n",
    "2. Encoded our object features\n",
    "3. Random Imputation for missing values\n",
    "4. Using a Decision Tree Classifier to help better predict those missing values\n",
    "5. Drop unnecessary features\n",
    "6. One Hot Encode all categorical features\n",
    "7. Scales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Combines test features and target for first transformation\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# 2. Drops rows with many missing entries\n",
    "X_test_mod, y_test_mod = Many_Nans_Row_Drop_FT.transform(test_df)\n",
    "\n",
    "\n",
    "# 3. Encoding\n",
    "# Creates dataframe of object types\n",
    "X_test_mod_obj = X_test_mod.select_dtypes(include='object').copy()\n",
    "# Creates dataframe of numerical features\n",
    "X_test_mod_num = X_test_mod.select_dtypes(exclude='object').copy()\n",
    "\n",
    "# Creates dataframe of ordinal encoded data\n",
    "X_test_mod_obj = pd.DataFrame(data = oe.transform(X_test_mod_obj),\n",
    "                                columns=X_test_mod_obj.columns,\n",
    "                                index=X_test_mod_obj.index)\n",
    "\n",
    "# Creates ordinal encoded set\n",
    "X_test_enc = pd.concat([X_test_mod_obj, X_test_mod_num], axis=1)\n",
    "\n",
    "# 4. Random Imputation for missing values\n",
    "X_test_imp = rand_imp.transform(X_test_enc)\n",
    "\n",
    "# 5. Regression to predict randomly imputed values\n",
    "X_test_trans = iter_class.transform(X_test_imp)\n",
    "\n",
    "# 5. Drops unnecessary features\n",
    "X_test_mod2 = X_test_trans.drop(drop_cols, axis=1).copy()\n",
    "\n",
    "# 6. One hot encodes categorical features\n",
    "X_test_ohe = pd.DataFrame(ct_ohe.transform(X_test_mod2),\n",
    "                          columns = ct_col_names,\n",
    "                          index=X_test_mod2.index)\n",
    "\n",
    "# 7. MinMaxScales data\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_ohe),\n",
    "                             columns=X_test_ohe.columns,\n",
    "                             index=X_test_ohe.index)\n",
    "\n",
    "# Checks shape to confirm size matches with training set\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Base Model\n",
    "\n",
    "Now that we have working datasets, let's begin the modeling process. Let's first create a basic model using a Decision Tree Classifier. We will cross validate the training set to prevent data leakage.\n",
    "\n",
    "For the scoring method to work properly, the target variables values have to be flipped since we're interested in predicting those who are not vaccinated.\n",
    "\n",
    "Therefore the values will be as followed, the old values are shown for reference:\n",
    "\n",
    "&emsp;Old: (Vaccinated = 1, Not-Vaccinated=0)<br>\n",
    "&emsp;New: (Vaccinated = 0, Not-Vaccinated=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_flip(ds):\n",
    "    '''\n",
    "    Takes in a pd.Series with [0,1] values and flips them.\n",
    "    ex. 0 to 1, 1 to 0\n",
    "    '''\n",
    "    return (ds+1).mod(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flips values of labels\n",
    "y_train_flipped = binary_flip(y_train_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Sets scores to be calculated\n",
    "scores = {'prec': 'precision',\n",
    "          'acc': 'accuracy',\n",
    "          'recall': 'recall',\n",
    "          'ROC': 'roc_auc'}\n",
    "\n",
    "cv_base_score = cross_validate(estimator=dt,\n",
    "                               X=X_train_scaled,\n",
    "                               y=y_train_flipped,\n",
    "                               scoring=scores,\n",
    "                               return_train_score=True)\n",
    "\n",
    "cv_base_scores_df = pd.DataFrame(cv_base_score)\n",
    "mean_df = pd.DataFrame(cv_base_scores_df.mean(), columns=['mean'])\n",
    "\n",
    "cv_base_scores_df = pd.concat([cv_base_scores_df.T, mean_df], axis=1)\n",
    "cv_base_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our base model appears to have performed pretty well but may be slightly over fit. There's a bit of variance between the folds. It's accuracy of 82.2% is around 4% better than if we guessed with the 78.7% majority that didn't receive the vaccine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently our base model has a precision of .855, meaning out of all our non-vaccinated predictions, 85.5% of them were correct. Our recall was .938 meaning we captured 93.3% of entries that weren't vaccinated. Though as mentioned before we want to concentrate our model qualification off of our precision, so let's try to improve this with future models.\n",
    "\n",
    "### Model 2\n",
    "\n",
    "For this model we will introduce an iterative approach through GridSearchCV. GridSearchCV helps by searching through combinations of hyperparameters for a given estimator. We will continue to use a DecisionTreeClassifier as our estimator at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DecisionTreeClassifier parameter grid\n",
    "dt_par_grid = {'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [4, 5, 6, 7, 8, 9, 10],\n",
    "            'min_samples_split': [.01, .025, .05, .1]}\n",
    "\n",
    "# Creates GridSearchCV object\n",
    "dt_clf_grid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
    "                   param_grid=dt_par_grid,\n",
    "                   scoring=scores,\n",
    "                   return_train_score = True,\n",
    "                   refit='prec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits training values to grid search object\n",
    "# dt_clf_grid.fit(X_train_scaled, y_train_flipped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves dt_clf_grid model to prevent long run times\n",
    "# with open('data/models/dt_clf_grid.plk', 'wb') as f:\n",
    "#     joblib.dump(dt_clf_grid, f)\n",
    "\n",
    "# Loads dt_clf_grid model to prevent long run times\n",
    "with open('data/models/dt_clf_grid.plk', 'rb') as f:\n",
    "    dt_clf_grid = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model from this grid search had a precision of around .86, only around .5% better than our base model. Let's check the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates DataFrame of results\n",
    "dt_clf_df = pd.DataFrame(dt_clf_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the model with the highest ranked precision score\n",
    "list(dt_clf_df.loc[dt_clf_df.rank_test_prec==1, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that multiple combinations of hyperparameters result in the same precision score. Observing the hyperparameter above, it seems like our models performed best when limited to a `min_sample_split` of around 5%. This parameter specifies the smallest leaf node sample size, in this case a leaf node can't have less than 5% of the samples. In the cases above all the models probably only had a depth of 4, resulting in the same score precision score. A higher rate of false positives must be getting predicted for the models able to split into smaller leaf nodes.\n",
    "\n",
    "Let's observe if these models were overfit by taking a look at the individual folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe of best scorer row and transposes it to a column\n",
    "best_scorer_dt = dt_clf_df.loc[dt_clf_df.rank_test_prec==1, :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays DataFrame containing fold scores, mean score \n",
    "# and std_dev for both training and test set\n",
    "myfunc.fold_scores(best_scorer_dt, 'prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train and test sets performed pretty closely with eachother and our folds variance wasn't large meaning we aren't over fitting. Let's check the rankings and other metrics for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List rankings and means of all scoring metrics\n",
    "myfunc.rankings(best_scorer_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models accuracy of around 82.3% and recall of 92.8% dropped slightly from our base models of 82.3% and 93.3% respectively. While our model performed the best in precision, this wasn't the case for all the other metrics. Let's create a figure observing the metrics in order of precision ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf_df.sort_values(by='rank_test_prec', inplace=True)\n",
    "\n",
    "score_vars = [col for col in dt_clf_df.columns if ('mean_test' in col)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6));\n",
    "\n",
    "for i, col in enumerate(score_vars):\n",
    "    ax.plot(dt_clf_df.rank_test_prec, dt_clf_df[col], label=col)\n",
    "\n",
    "ax.set_xlabel('Precision Score Ranking');\n",
    "ax.set_ylabel('Mean Score');\n",
    "ax.set_title('Mean Scores of Iterations by Order of Precision Score Ranking');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves current best models scores and hyperparameters for later reference\n",
    "best_model_scores = myfunc.rankings(best_scorer_dt).loc[:, 2]\n",
    "best_model_scores.rename('best_model', inplace=True)\n",
    "best_model_params = dt_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our precision drops our recall increases while our accuracy stays around the same. Even though we can see slight changes in the performance over these models, the scores only vary around one to two percentage points.\n",
    "\n",
    "So far this is our best model, but it wasn't much of an improvement over our base model. Therefore let's try a different modeling method.\n",
    "\n",
    "### Model 3\n",
    "\n",
    "For this model will try using gradient boosting with XGBoost. Boosting is an iterative method that starts by building a weak model, and learning from it. This process is repeated until a stopping condition is met and our model is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate XGBClassifier\n",
    "xgb_clf= XGBClassifier(random_state=42)\n",
    "\n",
    "# Creates parameter grid for search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, .3],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [.5, 1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100],\n",
    "}\n",
    "\n",
    "# creates gridsearch object using XGBoost as estimator\n",
    "xgb_clf_grid = GridSearchCV(xgb_clf, param_grid,\n",
    "                            scoring=scores,\n",
    "                            return_train_score = True,\n",
    "                            refit='prec')\n",
    "\n",
    "# fits xgb_clf to training data\n",
    "# xgb_clf_grid.fit(X_train_scaled, y_train_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves xgb_clf_grid model to prevent long run times\n",
    "# with open('data/models/xgb_clf_grid.plk', 'wb') as f:\n",
    "#     joblib.dump(xgb_clf_grid, f)\n",
    "\n",
    "# Loads xgb_clf_grid model to prevent long run times\n",
    "with open('data/models/xgb_clf_grid.plk', 'rb') as f:\n",
    "    xgb_clf_grid = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new models performance of 86% is almost identical to our current best model. Lets check to see if this model is overfitting and how well it performed with the other scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DataFrame of results\n",
    "clf_boost_df = pd.DataFrame(xgb_clf_grid.cv_results_)\n",
    "\n",
    "# Creates dataframe of best scorer row and trasposes it to a column\n",
    "best_scorer_booster = clf_boost_df.loc[clf_boost_df.rank_test_prec==1, :].T\n",
    "\n",
    "# displays DataFrame containing fold scores, mean score \n",
    "# and std_dev for both training and test set\n",
    "myfunc.fold_scores(best_scorer_booster, 'prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our model migh be slightly overfit with the training score being around 2% higher than our test scores. Let's take a look at how this models other scores compare to our current best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List rankings and means of all scoring metrics, and compares to current best model\n",
    "pd.concat([myfunc.rankings(best_scorer_booster), best_model_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model improved on the metrics from the current best model, but performs poorly in all metrics besides precision compared to the other models in this grid search. It also appears to be overfit with it's training metrics being atleast 2% better than it's test metrics. Since we are also interested in high accuracy let's check how the model with the highest accuracy performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe of best scorer row by accuracy and trasposes it to a column\n",
    "best_acc_booster = clf_boost_df.loc[clf_boost_df.rank_test_acc==1, :].T\n",
    "\n",
    "# displays DataFrame containing fold scores, mean score \n",
    "# and std_dev for both training and test set\n",
    "myfunc.fold_scores(best_acc_booster, 'prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models test precision score is almost identical to the top model by precision in this grid search, while also appearing to be better fit by being only .5% lower than the training precision score. The variance of the folds is also much smaller than the previous model. Let's this models other scores and how it compares to our current best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List rankings and means of all scoring metrics, and compares to current best model\n",
    "pd.concat([myfunc.rankings(best_acc_booster), best_model_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models test scores are much closer to the training scores than the top model in terms of precision. It also performed slightly better in terms of recall while being almost identical in terms of precision. It also out performs our current est model in terms of accuracy and recall increasing the scores from 82.3% to 83.4% and 92.8% to 94.2% respectively. Thus our best model will be updated to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves current best models scores and hyperparameters for later reference\n",
    "best_model_scores = myfunc.rankings(best_acc_booster)\n",
    "best_model_scores.rename(columns={best_model_scores.columns[0]: 'best_model'}, inplace=True)\n",
    "best_model_params = best_acc_booster.loc['params', best_acc_booster.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we still haven't seen much improvement on our base model, let's move on with another model.\n",
    "\n",
    "\n",
    "## Model 4 - Random Forest\n",
    "\n",
    "Random forest models combine multiple decision trees to decide the output. The output with the most decision trees predicting it is the one that is chosen overall for that entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42, bootstrap=True)\n",
    "\n",
    "param_grid_rf = {'n_estimators': (50, 100, 200, 400),\n",
    "                 'criterion': ('gini', 'entropy', 'log_loss'),\n",
    "                 'max_depth': (3, 4, 5, 6),\n",
    "                 'max_samples': (.3, .5, .7)}\n",
    "\n",
    "# creates gridsearch object using XGBoost as estimator\n",
    "rf_clf_grid = GridSearchCV(rf_clf, \n",
    "                           param_grid_rf,\n",
    "                           scoring=scores,\n",
    "                           return_train_score = True,\n",
    "                           refit='prec')\n",
    "\n",
    "# fits RandomForest Grid Search to training data\n",
    "# rf_clf_grid.fit(X_train_ohe, y_train_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saves rf_clf_grid model to prevent long run times\n",
    "# with open('data/models/rf_clf_grid.plk', 'wb') as f:\n",
    "#     joblib.dump(rf_clf_grid, f)\n",
    "\n",
    "# Loads rf_clf_grid model to prevent long run times\n",
    "with open('data/models/rf_clf_grid.plk', 'rb') as f:\n",
    "    rf_clf_grid = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random forest model performed worse than our previous best models with a precision score of 82%, down from around 86%. Let's take a look at the parameters and folds of our best random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DataFrame of results\n",
    "rf_clf_df = pd.DataFrame(rf_clf_grid.cv_results_)\n",
    "\n",
    "# Creates dataframe of best scorer row and trasposes it to a column\n",
    "best_scorer_rf = rf_clf_df.loc[rf_clf_df.rank_test_prec==1, :].T\n",
    "\n",
    "# displays DataFrame containing fold scores, mean score \n",
    "# and std_dev for both training and test set\n",
    "myfunc.fold_scores(best_scorer_rf, 'prec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random forest model isn't overfitting at the set hyperparameters. It performs best with a max depth of 6, with 200 estimators and while sampling half of our entries each time. Let's see how this model performed on the other scoring metrics compared against the current best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List rankings and means of all scoring metrics, and compares to current best model\n",
    "pd.concat([myfunc.rankings(best_scorer_rf), best_model_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model performed a few percentage points worse in precision than our best model so far, it has the highest recall score at .98, meaning it captures around 98% of our non-vaccinated entries. Our concentration is on the precision score of our model, since this one performed poorly in that regard it will not replace our best model.\n",
    "\n",
    "So far out models have been focused around types of decision trees, for our next model let's try something different.\n",
    "\n",
    "## Model 5\n",
    "\n",
    "For the next model we will try a Logistic Regression model. Similarly to linear regression, logistic regression uses a loss function to help predict the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "param_grid_lr = {'penalty': ('l1', 'l2'),\n",
    "                 'C': (.1, 1, 10),\n",
    "                 'solver': ['liblinear'],\n",
    "                 'max_iter': [200]}\n",
    "\n",
    "# creates gridsearch object using XGBoost as estimator\n",
    "lr_clf_grid = GridSearchCV(lr_clf, \n",
    "                           param_grid_lr,\n",
    "                           scoring=scores,\n",
    "                           return_train_score = True,\n",
    "                           refit='prec')\n",
    "\n",
    "# fits RandomForest Grid Search to training data\n",
    "# lr_clf_grid.fit(X_train_ohe, y_train_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves lr_clf_grid model to prevent long run times\n",
    "# with open('data/models/lr_clf_grid.plk', 'wb') as f:\n",
    "#     joblib.dump(lr_clf_grid, f)\n",
    "\n",
    "# # Loads lr_clf_grid model to prevent long run times\n",
    "with open('data/models/lr_clf_grid.plk', 'rb') as f:\n",
    "    lr_clf_grid = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model performed a little bit worse than our best model at 85.2% precision compared to 86%. Let's check its other scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DataFrame of results\n",
    "lr_clf_df = pd.DataFrame(lr_clf_grid.cv_results_)\n",
    "\n",
    "# Creates dataframe of best scorer row and trasposes it to a column\n",
    "best_scorer_lr = lr_clf_df.loc[lr_clf_df.rank_test_prec==1, :].T\n",
    "\n",
    "# List rankings and means of all scoring metrics, and compares to current best model\n",
    "pd.concat([myfunc.rankings(best_scorer_lr), best_model_scores], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scoring metrics to our best model, the logistic regression model performed worse except for recall. Therefore our best model will stay the same.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Final Model\n",
    "\n",
    "Our best model is the XGBoosted model with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this model on our full training set and check how it performs on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates XGBoost Classifier object with best model params\n",
    "final_model = XGBClassifier(learning_rate=.2,\n",
    "                            max_depth=3,\n",
    "                            min_child_weight=.5,\n",
    "                            n_estimators=100,\n",
    "                            subsample=.7,\n",
    "                            random_state=42)\n",
    "\n",
    "# fits classifier to training dataset\n",
    "final_model.fit(X_train_scaled, y_train_flipped);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves  final_model\n",
    "with open('data/models/final_model.plk', 'wb') as f:\n",
    "    joblib.dump(final_model, f)\n",
    "\n",
    "# # Loads final_modell to prevent long run times\n",
    "# with open('data/models/final_model.plk', 'rb') as f:\n",
    "#     final_model =joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds predicted values for train and test sets\n",
    "y_train_pred = final_model.predict(X_train_scaled)\n",
    "y_test_pred = final_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints out confusion matrix and metric report for training set\n",
    "myfunc.quick_metrics(y_train_flipped, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints out confusion matrix and metric report for test set\n",
    "y_test_flipped = binary_flip(y_test_mod)\n",
    "myfunc.quick_metrics(y_test_flipped, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training and test set scores match up pretty well so we know we aren't overfitting. This model also has the highest precision score out of all the models we have created, therefore fitting our goals. Testing for precision ensures that we have a high accuracy for those predicted to be unvaccinated. This is good for our business problem because we want to try and target the features whom strongly affect vaccination status. Let's now check which features hold the most importance in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = list(zip(final_model.feature_importances_, X_train_scaled.columns))\n",
    "feat_imp.sort(reverse=True)\n",
    "feat_imp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows the top 10 features in order of importance to our model. The top feature by a wide margin was `Det_doctor_recc_h1n1`. This agrees with the importance of a doctor recommendation that we saw in our data exploration stage, with a 53.3% vaccination rate in those who were recommended the vaccine compared to 13.6% of those who were not. Also agreeing with our earlier analysis is the importance of `Det_opinion_h1n1_vacc_effective` and `Det_opinion_h1n1_risk`. As the respondent's opinions on H1N1 risk and vaccination effectiveness increased so did the vaccination rate. \n",
    "\n",
    "The final model had a precision of 86% meaning of those predicted as unvaccinated 86% of them were correct. The total accuracy was also 84%, not much lower than our precision. According to the model, having a doctor recommend the vaccination had the greatest importance in one's decision. Followed by the respondent's opinions on the H1N1 vaccination effectiveness and H1N1 risk. Even though the vaccination rate was high in these groups the groups were much smaller then their counterparts. The vaccination rates for the groups who were recommended the vaccine, believed the vaccine was very effective or believed it was a high risk had vaccination rates between 40% - 54%. Though they only contained between 6% to 27% of the total amount of respondents.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pandemics are an ongoing concern for the entire population, with proper research our stakeholder is hoping to help in their prevention as well as increasing vaccination awareness. The goal was to find which features stand out the most in one's decision making process to receive a vaccination. The data available to us was a 2009 survey performed after the H1N1 pandemic. Our focus was to create a model with a high precision to ensure the correct target audience is located. Thus the best model in regards to precision will educate us on features that had the most weight in one's choice to get the vaccination. \n",
    "\n",
    "Observing the features that hold the most impact on the respondent's vaccination status, the conclusion that general practitioners hold the most weight in one's decision to be vaccinated can be obtained. As stated 53.3% of people with a doctors recommendation received the vaccine, compared to 13.6% of people who were not. Also it may be assumed that people receive information on vaccine effectiveness and virus risk from their personal health care physicians. Therefore I recommend reaching out to general practicioners with general information about vaccines or viruses to then transfer to their patients. This could possibly be in the form of pamphlets, posters or other sources of media. It should also be shown to the general practicioners how important their recommendation is in the outcome of their patient's decision. Only 22% of the respondant's in the survey were recommended the vaccine, yet more than half of those who were vaccinated were a part of that group.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "<br>\n",
    "<strong>Take a deeper look into the features that showed great importance.</strong> This includes finding subgroups in those features to help narrow down the target audience. For example we found out that those who believed the vaccination was effective had a much lower chance of being vaccinated, but could this be narrowed down to find the age group or other features that filled these sub groups more than others in order to find another group to target.<br><br>\n",
    "<strong>Fill out missing data about health insurance.</strong> A lot of entries were missing information on respondant's having access to health insurance so this feature was ommited. This feature may take a very important role in someone's decision to be vaccinated. As found getting a doctor's reccomendatin greatly improved the rate of someone recieving this vaccination, therefore if more people had access to primary physicians the population that recieves recommendations will increase and in turn the vaccination rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <a id='h1n1_cdc_article' href='https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html'>https://www.cdc.gov/flu/pandemic-resources/2009-h1n1-pandemic.html</a>\n",
    "\n",
    "[2] <a id='About the National Immunization Survery' href=\"https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1\">https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1</a>\n",
    "\n",
    "[3] <a href='https://www.drivendata.org/competitions/66/flu-shot-learning/data/'>https://www.drivendata.org/competitions/66/flu-shot-learning/data/</a>\n",
    "\n",
    "[4] <a id='Missing Data Imputation using Regression' href='https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression'>https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
